{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20496784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from app.vision_automl.ml_engine.trainer import FabricTrainer\n",
    "from app.vision_automl.ml_engine.model import ClassificationModel\n",
    "from app.vision_automl.ml_engine.datamodule import ClassificationData\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8299cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # -H \"Content-Type: multipart/form-data\" \\\n",
    "  # -F \"csv_file=@./sample_data/Garbage_Dataset_Classification/metadata.csv\" \\\n",
    "  # -F \"images_zip=@./sample_data/Garbage_Dataset_Classification/images.zip\" \\\n",
    "  # -F \"filename_column=filename\" \\\n",
    "  # -F \"label_column=label\" \\\n",
    "  # -F \"task_type=classification\" \\\n",
    "  # -F \"time_budget=30\")\n",
    "\n",
    "session_record = {\n",
    "  \"csv_file_path\" : \"/Users/smukherjee/Documents/Projects/ALFIE/alfie_automl_engine/uploaded_data/e42706fd-cdfe-4a42-818e-3363dd0b91b3/labels.csv\", \n",
    "  \"images_dir_path\": \"/Users/smukherjee/Documents/Projects/ALFIE/alfie_automl_engine/uploaded_data/e42706fd-cdfe-4a42-818e-3363dd0b91b3/images/images/\", \n",
    "  \"filename_column\": \"filename\", \n",
    "  \"label_column\": \"label\", \n",
    "  \"time_budget\" : 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef39448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "\n",
    "datamodule = ClassificationData(\n",
    "        csv_file=str(session_record[\"csv_file_path\"]),\n",
    "        root_dir=str(session_record[\"images_dir_path\"]),\n",
    "        img_col=str(session_record[\"filename_column\"]),\n",
    "        label_col=str(session_record[\"label_column\"]),\n",
    "        batch_size=64,\n",
    "    )\n",
    "\n",
    "# Train with FabricTrainer under a time budget\n",
    "# ensure proper type for time limit\n",
    "time_limit: float = float(cast(int, session_record[\"time_budget\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d368ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FabricTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        datamodule,\n",
    "        model_class,\n",
    "        model_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        optimizer_class=optim.AdamW,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        loss_fn: nn.Module = nn.CrossEntropyLoss(),\n",
    "        lr: float = 0.001,\n",
    "        epochs: int = 1,\n",
    "        time_limit: Optional[float] = None,\n",
    "        device: str = \"auto\",\n",
    "        callbacks: Optional[List[Any]] = None,\n",
    "        input_dtype: torch.dtype = torch.float32,\n",
    "        target_dtype: torch.dtype = torch.long,\n",
    "    ):\n",
    "        self.datamodule = datamodule\n",
    "        self.model_class = model_class\n",
    "        self.model_kwargs = model_kwargs or {}\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_kwargs = optimizer_kwargs or {\"lr\": lr}\n",
    "        self.loss_fn = loss_fn\n",
    "        self.epochs = epochs\n",
    "        self.time_limit = time_limit\n",
    "        self.device = device\n",
    "        self.callbacks = callbacks or []\n",
    "        self.input_dtype = input_dtype\n",
    "        self.target_dtype = target_dtype\n",
    "\n",
    "        self.fabric = L.Fabric(devices=self.device)\n",
    "        self._setup_model_optimizer()\n",
    "\n",
    "    def _setup_model_optimizer(self):\n",
    "        self.model = self.model_class(**self.model_kwargs)\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), **self.optimizer_kwargs)\n",
    "\n",
    "        train_loader = self.datamodule.train_dataloader()\n",
    "        val_loader = self.datamodule.val_dataloader()\n",
    "        self.model, self.optimizer, self.train_loader, self.val_loader = self.fabric.setup(\n",
    "            self.model, self.optimizer, train_loader, val_loader\n",
    "        )\n",
    "        self.test_loader = self.datamodule.test_dataloader()\n",
    "\n",
    "    def _move_batch(self, imgs, labels):\n",
    "        imgs = imgs.to(self.fabric.device, dtype=self.input_dtype)\n",
    "        labels = labels.to(self.fabric.device, dtype=self.target_dtype)\n",
    "        return imgs, labels\n",
    "\n",
    "    def _check_time_limit(self, start_time: float) -> bool:\n",
    "        if self.time_limit and (time.time() - start_time) > self.time_limit:\n",
    "            self.fabric.print(\"Time limit reached. Stopping training.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def train_epoch(self, epoch: int, start_time: float) -> float:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, labels in tqdm(self.train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "            if self._check_time_limit(start_time):\n",
    "                return running_loss / max(1, len(self.train_loader))\n",
    "            imgs, labels = self._move_batch(imgs, labels)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(imgs)\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "            self.fabric.backward(loss)\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        return running_loss / len(self.train_loader)\n",
    "\n",
    "    def validate(self, start_time: float):\n",
    "        self.model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.val_loader, desc=\"Validation\", leave=False):\n",
    "                if self._check_time_limit(start_time):\n",
    "                    break\n",
    "                imgs, labels = self._move_batch(imgs, labels)\n",
    "                outputs = self.model(imgs)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        avg_loss = val_loss / max(1, len(self.val_loader))\n",
    "        acc = correct / max(1, total)\n",
    "        self.fabric.print(f\"Val Loss: {avg_loss:.4f}, Val Acc: {acc:.4f}\")\n",
    "        return avg_loss, acc\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.test_loader, desc=\"Testing\"):\n",
    "                imgs, labels = self._move_batch(imgs, labels)\n",
    "                outputs = self.model(imgs)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        avg_loss = test_loss / len(self.test_loader)\n",
    "        acc = correct / total\n",
    "        self.fabric.print(f\"\\nTest Loss: {avg_loss:.4f}, Test Acc: {acc:.4f}\")\n",
    "        return avg_loss, acc\n",
    "\n",
    "    def fit(self):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self.train_epoch(epoch, start_time)\n",
    "            val_loss, val_acc = self.validate(start_time)\n",
    "            logs = {\"train_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": val_acc}\n",
    "            for cb in self.callbacks:\n",
    "                cb.on_epoch_end(self, epoch, logs)\n",
    "            if self._check_time_limit(start_time):\n",
    "                break\n",
    "        return self.test()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af478e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "def export_to_onnx(model):\n",
    "    return torch.onnx.export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b5dd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(datamodule.train_dataloader()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10f578f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = inputs[1].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbf44b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flush'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/onnx/_internal/onnx_proto_utils.py:172\u001b[39m, in \u001b[36m_export_file\u001b[39m\u001b[34m(model_bytes, f, export_map)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.serialization._open_file_like(f, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[43mopened_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m(model_bytes)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'write'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# If inputs[1] is shape [3, 224, 224], make it [1, 3, 224, 224]\u001b[39;00m\n\u001b[32m      4\u001b[39m dummy = inputs[\u001b[32m1\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdummy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# \"model.onnx\",\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/onnx/__init__.py:383\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    379\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe exporter only supports dynamic shapes \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    381\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/onnx/utils.py:495\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    493\u001b[39m     args = args + (kwargs,)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/onnx/utils.py:1507\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m   1505\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1506\u001b[39m             _C._jit_onnx_log(\u001b[33m\"\u001b[39m\u001b[33mExported graph: \u001b[39m\u001b[33m\"\u001b[39m, graph)\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m         \u001b[43monnx_proto_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_export_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m GLOBALS.in_onnx_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/onnx/_internal/onnx_proto_utils.py:171\u001b[39m, in \u001b[36m_export_file\u001b[39m\u001b[34m(model_bytes, f, export_map)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"export/write model bytes into directory/protobuf/zip\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(export_map) == \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/serialization.py:746\u001b[39m, in \u001b[36m_open_buffer_writer.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflush\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'flush'"
     ]
    }
   ],
   "source": [
    "model = trainer.model.to(torch.device('cpu')).eval()\n",
    "\n",
    "# If inputs[1] is shape [3, 224, 224], make it [1, 3, 224, 224]\n",
    "dummy = inputs[1].unsqueeze(0)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy,\n",
    "    \"model.onnx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "373ce5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "def get_num_params_if_available(repo_id: str, revision: str | None = None):\n",
    "    api = HfApi()\n",
    "    info = api.model_info(repo_id, revision=revision, files_metadata=True)\n",
    "    num_params = getattr(info, \"safetensors\", None)\n",
    "    if num_params is not None:\n",
    "        return num_params.total\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def search_hf_for_pytorch_models_with_estimated_parameters(filter = \"image-classification\",limit: int = 3, sort = \"downloads\"):\n",
    "    api = HfApi()\n",
    "    models = api.list_models(\n",
    "        filter=filter,\n",
    "        library=\"pytorch\",\n",
    "        sort=sort, \n",
    "        direction=-1,        \n",
    "        limit=limit,\n",
    "    )\n",
    "    \n",
    "    model_list_pass_one = [\n",
    "        {\n",
    "            \"model_id\": m.id,\n",
    "            \"downloads\": getattr(m, \"downloads\", None),\n",
    "            \"likes\": getattr(m, \"likes\", None),\n",
    "            \"last_modified\": getattr(m, \"lastModified\", None),\n",
    "            \"private\": getattr(m, \"private\", None),\n",
    "            \"num_params\" : get_num_params_if_available(m.id)\n",
    "        }\n",
    "        for m in models\n",
    "    ]\n",
    "\n",
    "    # cleaned_up_list_iff_params\n",
    "    return [model for model in model_list_pass_one if model[\"num_params\"] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9e48cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "source": [
    "candidate_models = search_hf_for_pytorch_models_with_estimated_parameters(filter = \"image-classification\", limit = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da04956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       "  'downloads': 367358,\n",
       "  'likes': 4,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 88337896},\n",
       " {'model_id': 'google/vit-base-patch16-224',\n",
       "  'downloads': 3306984,\n",
       "  'likes': 858,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 86567656},\n",
       " {'model_id': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       "  'downloads': 373484,\n",
       "  'likes': 9,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 86567656},\n",
       " {'model_id': 'nateraw/vit-age-classifier',\n",
       "  'downloads': 454519,\n",
       "  'likes': 138,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85805577},\n",
       " {'model_id': 'trpakov/vit-face-expression',\n",
       "  'downloads': 7903628,\n",
       "  'likes': 78,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85804039},\n",
       " {'model_id': 'Falconsai/nsfw_image_detection',\n",
       "  'downloads': 99558944,\n",
       "  'likes': 811,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'rizvandwiki/gender-classification',\n",
       "  'downloads': 1095566,\n",
       "  'likes': 51,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'rizvandwiki/gender-classification-2',\n",
       "  'downloads': 542649,\n",
       "  'likes': 34,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/resnet34.a1_in1k',\n",
       "  'downloads': 493656,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 21814696},\n",
       " {'model_id': 'timm/tf_efficientnetv2_s.in21k_ft_in1k',\n",
       "  'downloads': 402797,\n",
       "  'likes': 1,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 21612360},\n",
       " {'model_id': 'timm/resnet18.a1_in1k',\n",
       "  'downloads': 2729822,\n",
       "  'likes': 12,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 11699112},\n",
       " {'model_id': 'timm/resnet18.a3_in1k',\n",
       "  'downloads': 597367,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 11699112},\n",
       " {'model_id': 'timm/vit_tiny_patch16_224.augreg_in21k',\n",
       "  'downloads': 340263,\n",
       "  'likes': 1,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 9740115},\n",
       " {'model_id': 'timm/efficientnet_b0.ra_in1k',\n",
       "  'downloads': 397869,\n",
       "  'likes': 4,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 5330564},\n",
       " {'model_id': 'timm/convnext_femto.d1_in1k',\n",
       "  'downloads': 747507,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 5217784},\n",
       " {'model_id': 'timm/mobilenetv3_small_100.lamb_in1k',\n",
       "  'downloads': 112183087,\n",
       "  'likes': 34,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 2554968}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "newlist = sorted(candidate_models, key=itemgetter('num_params'), reverse = True)\n",
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f54febbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'timm/mobilenetv3_small_100.lamb_in1k',\n",
       "  'downloads': 112183087,\n",
       "  'likes': 34,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 2554968},\n",
       " {'model_id': 'Falconsai/nsfw_image_detection',\n",
       "  'downloads': 99558944,\n",
       "  'likes': 811,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/resnet50.a1_in1k',\n",
       "  'downloads': 8285773,\n",
       "  'likes': 39,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'trpakov/vit-face-expression',\n",
       "  'downloads': 7903628,\n",
       "  'likes': 78,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85804039},\n",
       " {'model_id': 'google/vit-base-patch16-224',\n",
       "  'downloads': 3306984,\n",
       "  'likes': 858,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 86567656},\n",
       " {'model_id': 'timm/resnet18.a1_in1k',\n",
       "  'downloads': 2729822,\n",
       "  'likes': 12,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 11699112},\n",
       " {'model_id': 'apple/mobilevit-small',\n",
       "  'downloads': 1350424,\n",
       "  'likes': 80,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'rizvandwiki/gender-classification',\n",
       "  'downloads': 1095566,\n",
       "  'likes': 51,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/convnext_femto.d1_in1k',\n",
       "  'downloads': 747507,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 5217784},\n",
       " {'model_id': 'timm/resnet18.a3_in1k',\n",
       "  'downloads': 597367,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 11699112},\n",
       " {'model_id': 'rizvandwiki/gender-classification-2',\n",
       "  'downloads': 542649,\n",
       "  'likes': 34,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/resnet34.a1_in1k',\n",
       "  'downloads': 493656,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 21814696},\n",
       " {'model_id': 'nateraw/vit-age-classifier',\n",
       "  'downloads': 454519,\n",
       "  'likes': 138,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85805577},\n",
       " {'model_id': 'microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
       "  'downloads': 416907,\n",
       "  'likes': 77,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'microsoft/swinv2-tiny-patch4-window16-256',\n",
       "  'downloads': 403717,\n",
       "  'likes': 8,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'timm/tf_efficientnetv2_s.in21k_ft_in1k',\n",
       "  'downloads': 402797,\n",
       "  'likes': 1,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 21612360},\n",
       " {'model_id': 'timm/efficientnet_b0.ra_in1k',\n",
       "  'downloads': 397869,\n",
       "  'likes': 4,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 5330564},\n",
       " {'model_id': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       "  'downloads': 373484,\n",
       "  'likes': 9,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 86567656},\n",
       " {'model_id': 'timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       "  'downloads': 367358,\n",
       "  'likes': 4,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 88337896},\n",
       " {'model_id': 'timm/vit_tiny_patch16_224.augreg_in21k',\n",
       "  'downloads': 340263,\n",
       "  'likes': 1,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 9740115}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9a34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'timm/mobilenetv3_small_100.lamb_in1k',\n",
       "  'downloads': 112183087,\n",
       "  'likes': 34,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 2554968},\n",
       " {'model_id': 'Falconsai/nsfw_image_detection',\n",
       "  'downloads': 99558944,\n",
       "  'likes': 811,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/resnet50.a1_in1k',\n",
       "  'downloads': 8285773,\n",
       "  'likes': 39,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'trpakov/vit-face-expression',\n",
       "  'downloads': 7903628,\n",
       "  'likes': 78,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85804039},\n",
       " {'model_id': 'google/vit-base-patch16-224',\n",
       "  'downloads': 3306984,\n",
       "  'likes': 858,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 86567656},\n",
       " {'model_id': 'timm/resnet18.a1_in1k',\n",
       "  'downloads': 2729822,\n",
       "  'likes': 12,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 11699112},\n",
       " {'model_id': 'apple/mobilevit-small',\n",
       "  'downloads': 1350424,\n",
       "  'likes': 80,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'rizvandwiki/gender-classification',\n",
       "  'downloads': 1095566,\n",
       "  'likes': 51,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/convnext_femto.d1_in1k',\n",
       "  'downloads': 747507,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 5217784},\n",
       " {'model_id': 'timm/resnet18.a3_in1k',\n",
       "  'downloads': 597367,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 11699112},\n",
       " {'model_id': 'rizvandwiki/gender-classification-2',\n",
       "  'downloads': 542649,\n",
       "  'likes': 34,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85800194},\n",
       " {'model_id': 'timm/resnet34.a1_in1k',\n",
       "  'downloads': 493656,\n",
       "  'likes': 0,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 21814696},\n",
       " {'model_id': 'nateraw/vit-age-classifier',\n",
       "  'downloads': 454519,\n",
       "  'likes': 138,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 85805577},\n",
       " {'model_id': 'microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
       "  'downloads': 416907,\n",
       "  'likes': 77,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'microsoft/swinv2-tiny-patch4-window16-256',\n",
       "  'downloads': 403717,\n",
       "  'likes': 8,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': None},\n",
       " {'model_id': 'timm/tf_efficientnetv2_s.in21k_ft_in1k',\n",
       "  'downloads': 402797,\n",
       "  'likes': 1,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 21612360},\n",
       " {'model_id': 'timm/efficientnet_b0.ra_in1k',\n",
       "  'downloads': 397869,\n",
       "  'likes': 4,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 5330564},\n",
       " {'model_id': 'timm/vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       "  'downloads': 373484,\n",
       "  'likes': 9,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 86567656},\n",
       " {'model_id': 'timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       "  'downloads': 367358,\n",
       "  'likes': 4,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 88337896},\n",
       " {'model_id': 'timm/vit_tiny_patch16_224.augreg_in21k',\n",
       "  'downloads': 340263,\n",
       "  'likes': 1,\n",
       "  'last_modified': None,\n",
       "  'private': False,\n",
       "  'num_params': 9740115}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68e618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e335a40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SafeTensorsInfo(parameters={'F32': 86567656}, total=86567656)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_model_size_mb(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa653120",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'id': 'google/vit-base-patch16-224', 'author': 'google', 'sha': '3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3', 'last_modified': datetime.datetime(2023, 9, 5, 15, 27, 12, tzinfo=datetime.timezone.utc), 'created_at': datetime.datetime(2022, 3, 2, 23, 29, 5, tzinfo=datetime.timezone.utc), 'private': False, 'gated': False, 'disabled': False, 'downloads': 3306984, 'downloads_all_time': None, 'likes': 858, 'library_name': 'transformers', 'gguf': None, 'inference': None, 'inference_provider_mapping': None, 'tags': ['transformers', 'pytorch', 'tf', 'jax', 'safetensors', 'vit', 'image-classification', 'vision', 'dataset:imagenet-1k', 'dataset:imagenet-21k', 'arxiv:2010.11929', 'arxiv:2006.03677', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], 'pipeline_tag': 'image-classification', 'mask_token': None, 'trending_score': None, 'card_data': {'base_model': None, 'datasets': ['imagenet-1k', 'imagenet-21k'], 'eval_results': None, 'language': None, 'library_name': None, 'license': 'apache-2.0', 'license_name': None, 'license_link': None, 'metrics': None, 'model_name': None, 'pipeline_tag': None, 'tags': ['vision', 'image-classification'], 'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg', 'example_title': 'Tiger'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg', 'example_title': 'Teapot'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg', 'example_title': 'Palace'}]}, 'widget_data': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg', 'example_title': 'Tiger'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg', 'example_title': 'Teapot'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg', 'example_title': 'Palace'}], 'model_index': None, 'config': {'architectures': ['ViTForImageClassification'], 'model_type': 'vit'}, 'transformers_info': TransformersInfo(auto_model='AutoModelForImageClassification', custom_class=None, pipeline_tag='image-classification', processor='AutoImageProcessor'), 'siblings': [RepoSibling(rfilename='.gitattributes', size=744, blob_id='2514e63570ec0eb13d617d6a316ac52e9eed9746', lfs=None), RepoSibling(rfilename='README.md', size=5688, blob_id='550f72aac12a408fe77b7a2fdebc85057a77c0d1', lfs=None), RepoSibling(rfilename='config.json', size=69665, blob_id='21c61d3f8f3e50137a8c5fdd5bc9b085e286315a', lfs=None), RepoSibling(rfilename='flax_model.msgpack', size=346277993, blob_id='5f079399d5c4a8af31f21a05ad6913ea1a04f532', lfs=BlobLfsInfo(size=346277993, sha256='e0c809a1fe1d79c51c4da8a9ebd6c0923bac317ca469a33c6620977054149471', pointer_size=134)), RepoSibling(rfilename='model.safetensors', size=346293852, blob_id='d1e886a1fdb46f99c6cf9e603ed1c22fbf5c8453', lfs=BlobLfsInfo(size=346293852, sha256='1cea07110a4a47edc51420b2dda6f3b8b58e7256e8f44b4ea6aa9696162ccb5d', pointer_size=134)), RepoSibling(rfilename='preprocessor_config.json', size=160, blob_id='70fbc148eb26a06bac351d46fddc0a23037b4ce4', lfs=None), RepoSibling(rfilename='pytorch_model.bin', size=346351599, blob_id='a74020ed0ffd32c10585c3c21885b1a9e7f97d53', lfs=BlobLfsInfo(size=346351599, sha256='5f17067668129d23b52524f90a805e7d9914c276d90a59a13ebe81a09e40ceca', pointer_size=134)), RepoSibling(rfilename='tf_model.h5', size=346537664, blob_id='7665bbafc2d89b40ba2fa854a124762bfa528ccb', lfs=BlobLfsInfo(size=346537664, sha256='30f9125423060139b80ddae09daa8a1b612eb1eda8fc34a0b58cdfe920cbbc0f', pointer_size=134))], 'spaces': ['gunship999/SexyImages', 'Yntec/ToyWorld', 'llamameta/flux-pro-uncensored', 'Uthar/SexyReality', 'Yntec/PrintingPress', 'Nymbo/Compare-6', 'M2UGen/M2UGen-Demo', 'llamameta/fluxproV2', 'phenixrhyder/NSFW-ToyWorld', 'Yntec/ToyWorldXL', 'burtenshaw/autotrain-mcp', 'Yntec/blitz_diffusion', 'John6666/Diffusion80XX4sg', 'John6666/PrintingPress4', 'llamameta/fast-sd3.5-large', 'martynka/TasiaExperiment', 'yergyerg/ImgGenClone', 'Yntec/Image-Models-Test-2024', 'Nuno-Tome/simple_image_classifier', 'Yntec/Image-Models-Test-April-2024', 'DemiPoto/TestDifs', 'Abinivesh/Multi-models-prompt-to-image-generation', 'team-indain-image-caption/Hindi-image-captioning', 'tonyassi/product-recommendation', 'NativeAngels/Compare-6', 'abidlabs/vision-transformer', 'John6666/hfd_test_nostopbutton', 'Nymbo/Diffusion80XX4sg', 'DemiPoto/testSortModels', 'kaleidoskop-hug/PrintingPress', 'autonomous019/image_story_generator', 'Chakshu123/image-colorization-with-hint', 'Somnath3570/food_calories', 'Npps/Food_Indentification_and_Nutrition_Info', 'Yntec/MiniToyWorld', 'John6666/ToyWorld4', 'Ramos-Ramos/visual-emb-gam-probing', 'Chakshu123/sketch-colorization-with-hint', 'John6666/Diffusion80XX4g', 'SAITAN666/StableDiffusion35Large-Image-Models-Test-November-2024', 'NativeAngels/HuggingfaceDiffusion', 'Yntec/Image-Models-Test-December-2024', 'abidlabs/image-classifier', 'hysts/space-that-creates-model-demo-space', 'st0bb3n/Cam2Speech', 'jamesgray007/berkeley-ai-m3', 'John6666/Diffusion80XX4', 'K00B404/HuggingfaceDiffusion_custom', 'John6666/blitz_diffusion4', 'John6666/blitz_diffusion_builtin', 'eksemyashkina/clothes-segmentation', 'K00B404/SimpleBrothel', 'j0hngou/vision-diffmask', 'ipvikas/ImageProcessing', 'HighCWu/anime-colorization-with-hint', 'ClassCat/ViT-ImageNet-Classification', 'Yntec/Image-Models-Test-July-2024', 'Blane187/multi-diffusion', 'NativeAngels/ToyWorld', 'Uthar/LewdExperiments', 'Uthar/BodyPaint', 'Uthar/HRGiger', 'Uthar/HighFashion', 'Yntec/open-craiyon', 'Yntec/Image-Models-Test-March-2025', 'mmeendez/cnn_transformer_explainability', 'nickmuchi/Plant-Health-Classifier', 'Saiteja/leaf-ViT-classifier', 'dreamdrop-art/000555111', 'awacke1/MusicChatGenWithMuGen', 'Nuno-Tome/bulk_image_classifier', 'LucyintheSky/sketch-to-dress', 'andreped/vit-explainer', 'Somnath3570/food_calories_calculation', 'Shiladitya123Mondal/Food-Nutrition-app', 'swdqwewfw/Calorie_Calculator', 'Yeeezus/SexyImages', 'SharafeevRavil/test', 'John6666/MiniToyWorld', 'bryantmedical/oral_cancer', 'yiw/text', 'ThankGod/image-classifier', 'autonomous019/Story_Generator_v2', 'IPN/demo_', 'webis-huggingface-workshop/omar_demo', 'vebie91/spaces-image-classification-demo', 'suresh-subramanian/bean-classification', 'akhaliq/space-that-creates-model-demo-space', 'paschalc/ImageRecognitionDemo', 'peteralexandercharles/space-that-creates-model-demo-space', 'awacke1/MultiplayerImageRecognition-Gradio', 'mushroomsolutions/Gallery', 'xxx1/VQA_CAP_GPT', 'Kluuking/google-vit-base', 'Megareyka/imageRecognition', 'samavi/openai-clip-vit-base-patch32', 'HaawkeNeural/google-vit-base-patch16-224', 'jordonpeter01/M2UGen-Super-30s', 'JCRios/demo_image_classification', 'mrolando/classify_images'], 'safetensors': SafeTensorsInfo(parameters={'F32': 86567656}, total=86567656), 'security_repo_status': None, 'xet_enabled': None, 'lastModified': datetime.datetime(2023, 9, 5, 15, 27, 12, tzinfo=datetime.timezone.utc), 'cardData': {'base_model': None, 'datasets': ['imagenet-1k', 'imagenet-21k'], 'eval_results': None, 'language': None, 'library_name': None, 'license': 'apache-2.0', 'license_name': None, 'license_link': None, 'metrics': None, 'model_name': None, 'pipeline_tag': None, 'tags': ['vision', 'image-classification'], 'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg', 'example_title': 'Tiger'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg', 'example_title': 'Teapot'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg', 'example_title': 'Palace'}]}, 'transformersInfo': TransformersInfo(auto_model='AutoModelForImageClassification', custom_class=None, pipeline_tag='image-classification', processor='AutoImageProcessor'), '_id': '621ffdc136468d709f17b7d7', 'modelId': 'google/vit-base-patch16-224', 'usedStorage': 2550907501}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0bfcfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "test_model_id = candidate_models[0]['model_id']\n",
    "processor = AutoImageProcessor.from_pretrained(test_model_id)\n",
    "model = AutoModelForImageClassification.from_pretrained(test_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3ceb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimmWrapperForImageClassification(\n",
       "  (timm_model): MobileNetV3(\n",
       "    (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNormAct2d(\n",
       "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): Hardswish()\n",
       "    )\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): ReLU(inplace=True)\n",
       "          )\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_expand): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Hardsigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): ConvBnAct(\n",
       "          (conv): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Hardswish()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (conv_head): Conv2d(576, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (act2): Hardswish()\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8502e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "class HFImageClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_model_name: str = \"google/vit-base-patch16-224\",\n",
    "        num_classes: int = 2,\n",
    "        id2label: dict | None = None,\n",
    "        label2id: dict | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForImageClassification.from_pretrained(\n",
    "            hf_model_name,\n",
    "            num_labels=num_classes,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.model(pixel_values=x)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d7936",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer = \u001b[43mFabricTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# upper bound; time_limit will cut earlier\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# test_loss, test_acc = trainer.fit()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mFabricTrainer.__init__\u001b[39m\u001b[34m(self, datamodule, model_class, model_kwargs, optimizer_class, optimizer_kwargs, loss_fn, lr, epochs, time_limit, device, callbacks, input_dtype, target_dtype)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.target_dtype = target_dtype\n\u001b[32m     39\u001b[39m \u001b[38;5;28mself\u001b[39m.fabric = L.Fabric(devices=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_model_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mFabricTrainer._setup_model_optimizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_setup_model_optimizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[38;5;28mself\u001b[39m.optimizer_class(\u001b[38;5;28mself\u001b[39m.model.parameters(), **\u001b[38;5;28mself\u001b[39m.optimizer_kwargs)\n\u001b[32m     46\u001b[39m     train_loader = \u001b[38;5;28mself\u001b[39m.datamodule.train_dataloader()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ALFIE/alfie_automl_engine/.venv/lib/python3.11/site-packages/transformers/models/timm_wrapper/modeling_timm_wrapper.py:321\u001b[39m, in \u001b[36mTimmWrapperForImageClassification.forward\u001b[39m\u001b[34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.timm_model, \u001b[33m\"\u001b[39m\u001b[33mforward_intermediates\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    315\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m'\u001b[39m\u001b[33m option cannot be set for this timm model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo enable this feature, the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mforward_intermediates\u001b[39m\u001b[33m'\u001b[39m\u001b[33m method must be implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min the timm model (available in timm versions > 1.*). Please consider using a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdifferent architecture or updating the timm package to a compatible version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m pixel_values = \u001b[43mpixel_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# to enable hidden states selection\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_hidden_states, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model = HFImageClassifier(\n",
    "    hf_model_name=\"google/vit-base-patch16-224\",\n",
    "    num_classes=len(train_ds.classes),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer = FabricTrainer(\n",
    "    datamodule=datamodule,\n",
    "    model_class=model,\n",
    "    model_kwargs={\n",
    "        \"pixel_values\": None\n",
    "    },\n",
    "    optimizer_class=optim.AdamW,\n",
    "    optimizer_kwargs={\"lr\": 0.001},\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    epochs=50,  # upper bound; time_limit will cut earlier\n",
    "    time_limit=time_limit,\n",
    ")\n",
    "# test_loss, test_acc = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80cd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
