{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ALFIE AutoML Engine","text":"<p>An AutoML engine written for the ALFIE project with the following features - AutoML for tabular - AutoML for vision (this is a WIP) - Website accessibility checker - Image to website tool - Part of a host of tools made for better and more informed generation of AI models</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#installation","title":"Installation","text":"<ul> <li>First, follow the steps here, installation</li> <li>Then, follow this to install AutoDW</li> </ul>"},{"location":"#generate-sample-data-optional","title":"Generate sample data (optional)","text":"<p>The repo includes <code>mk_sample_data.sh</code> which downloads small datasets under <code>sample_data/</code>: <pre><code># From project root\nbash mk_sample_data.sh\n</code></pre> What it fetches: - <code>sample_data/knot_theory/{train.csv,test.csv}</code> - <code>sample_data/m4_hourly_subset/{train.csv,test.csv}</code></p> <p>If <code>wget</code> is missing on macOS: <code>brew install wget</code>.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>You can set environment variables via the <code>.env</code> file in the project root.</p> <ul> <li>Copy the <code>.env.template</code> to <code>.env</code> and fill in whatever is missing</li> <li>Change the ports if needed</li> <li>Uploads are saved under <code>uploaded_data/</code>.</li> <li>AutoML artifacts (from training) are written alongside the uploaded session folder in <code>automl_data_path/</code>.</li> </ul>"},{"location":"#running-the-automl","title":"Running the AutoML","text":"<ul> <li>Once you have everything setup, and the data uploaded to AutoDW, you can use the AUotML!<ul> <li>Note that if you only want to use AutoMLPlus, you don't need to use AutoDW or have it running</li> </ul> </li> <li>Just follow this to run the service of your choosing</li> </ul>"},{"location":"#system-components","title":"System components","text":""},{"location":"#loading-the-trained-model-for-inference","title":"Loading the trained model for inference","text":"<ul> <li>Follow the instructions here</li> <li>Once the AutoML tool is done, it will point you to a folder with </li> </ul>"},{"location":"autodw/","title":"AutoDW Setup","text":"<ul> <li>Clone the repository : https://github.com/sstamatis01/ALFIE-Data-Warehouse</li> <li>cd into it</li> <li>Do a <code>docker-compose up -d</code></li> <li>Open the swagger UI : <code>http://localhost:8000/docs#/</code><ul> <li>Upload a dataset : POST [/dataset/upload/{user_id}] (Click try it out and fill in the details)</li> <li>Note the user id and dataset id of course, then you can use it with the AutoML tool</li> </ul> </li> <li>!Note: All this is only temporary, once the actual service is up and running, there will be a proper UI for everything</li> </ul>"},{"location":"docker_instructions/","title":"Docker instructions","text":""},{"location":"docker_instructions/#running-everything","title":"Running everything","text":"<ul> <li>Simply do <code>docker compose up</code> in the main folder (assuming you have docker installed)</li> <li>After that you can <code>curl</code> any of the services you want</li> <li>For information on the ports, please look at your .env file</li> <li>eg: Website Accessibility (HTML file input)</li> </ul> <pre><code>curl -s -X POST \"http://localhost:8001/automl_tabular/best_model/\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"user_id=1\" \\\n  -F \"dataset_id=2\" \\\n  -F \"target_column_name=signature\" \\\n  -F \"task_type=classification\" \\\n  -F \"time_stamp_column_name=\" \\\n  -F \"time_budget=30\"\n</code></pre>"},{"location":"docker_instructions/#pushing-to-repo","title":"Pushing to repo","text":"<p>Login: echo {PASS}  | docker login gitlab.catalink.eu:5050 -u {USER} --password-stdin Build &amp; tag: docker build -t gitlab.catalink.eu:5050/external/alfie_eu/alfie/{MODULE}:{TAG} Push: docker push gitlab.catalink.eu:5050/external/alfie_eu/alfie/{MODULE}:{TAG}</p>"},{"location":"installation/","title":"Installation Instructions","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11</li> <li>uv (recommended) or pip</li> <li>bash and wget (for sample data script; on macOS: <code>brew install wget</code>)</li> </ul>"},{"location":"installation/#environment-and-dependencies-recommended-uv","title":"Environment and dependencies (recommended: uv)","text":"<pre><code># Install uv (user scope)\npip install --user uv\n\n# From project root, create and activate a virtual env\nuv venv --seed --python 3.11 .venv\nsource .venv/bin/activate\n\n# Install project dependencies\n# Option A: Use pyproject + uv.lock (preferred if `uv.lock` is present)\nuv sync --frozen\n\n# Option B: Sync from requirements.txt\nuv pip sync requirements.txt\n</code></pre> <p>Alternative with pip only: <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre></p>"},{"location":"installation/#autodw","title":"!!AutoDW","text":"<ul> <li>Another important package we need is AutoDW, follow this for instructions</li> </ul>"},{"location":"installation/#managing-packages-with-uv","title":"Managing packages with uv","text":"<p>Use uv to manage dependencies declared in <code>pyproject.toml</code> (lockfile: <code>uv.lock</code>). Common tasks: <pre><code># Add a runtime dependency (updates pyproject and lockfile)\nuv add requests\n\n# Remove a dependency\nuv remove requests\n\n# Upgrade all dependencies to latest allowed by constraints\nuv lock --upgrade\nuv sync\n\n# Install dev-only tools (use --dev group if you have groups defined)\nuv add --dev ruff\n</code></pre></p>"},{"location":"loading_model/","title":"Loading a model","text":"<ul> <li>Once the AutoML tool is done, it will point you to a folder with the results/or you can find the results.zip from AutoDW</li> </ul>"},{"location":"loading_model/#tabular-dataset","title":"Tabular dataset","text":"<ul> <li>Once you have this folder (extract the zip if it is one), you can simply do <pre><code>from autogluon.tabular import TabularPredictor\npredictor = TabularPredictor.load(predictor_path)\n</code></pre></li> <li>Now to predict something you can do, <code>predictor.predict(test_data)</code> or <code>predictor.predict(test_data, model = 'X')</code> for a specific model</li> <li>For more instructions on how to use this, please refer to the AutoGluon documentation page</li> </ul>"},{"location":"loading_model/#vision-dataset","title":"Vision Dataset","text":"<ul> <li>This is a WIP</li> </ul>"},{"location":"running_the_services/","title":"Running and Testing AutoML Services","text":"<p>This guide shows how to start each ALFIE service using <code>uvicorn</code> directly from the shell, and how to test them with <code>curl</code>.</p> <ul> <li>This page lists the services that exist at the moment and the options available for each</li> <li>Note that this might change in the future</li> <li>IMPORTANT : The localhost url might change in the future and is not loaded automatically from the .env here, so if something doesnt work, probably look at that first?</li> </ul>"},{"location":"running_the_services/#prerequisites","title":"Prerequisites","text":"<ul> <li>You followed the setup</li> <li>You have the azure keys (for AutoML plus tasks)</li> <li>AutoDW is running (check AutoDW Setup)</li> </ul>"},{"location":"running_the_services/#killing-a-service-on-a-port","title":"Killing a Service on a Port","text":"<p>If a port is stuck, kill any process using it:</p> <pre><code>lsof -ti tcp:8000 | xargs kill -9\n</code></pre> <p>Replace <code>8000</code> with the relevant service port.</p>"},{"location":"running_the_services/#services-overview-this-might-change-based-on-the-env-file","title":"Services Overview (This might change based on the .env file)","text":"<p>To run this locally, run each service you want to test in its own shell using (replace from table):</p> <pre><code>uv run uvicorn app.x.main:app --reload --host 0.0.0.0 --port 800x\n</code></pre> Service Port Uvicorn Target Description webfromfile 8003 <code>app.automlplus.main:app</code> Website accessibility (HTML file input) webfromurl 8003 <code>app.automlplus.main:app</code> Website accessibility (URL input) im2web 8003 <code>app.automlplus.main:app</code> Image-to-Website tool tabular 8001 <code>app.tabular_automl.main:app</code> AutoML for tabular datasets vision 8002 <code>app.vision_automl.main:app</code> AutoML for vision datasets AutoDW 8000 <code>autodw service</code> AutoDW"},{"location":"running_the_services/#testing-services-with-curl","title":"Testing Services with Curl","text":"<p>Open another shell and run the tests.</p>"},{"location":"running_the_services/#automl-plus","title":"AutoML Plus","text":"<p>Bits and bobs that are not really \"AutoML\" but use AI models for a specific use case</p>"},{"location":"running_the_services/#test-website-accessibility-html-file-input","title":"Test: Website Accessibility (HTML file input)","text":"<ul> <li>This tests the accessibility of a file given an HTML</li> <li>Only options here are to enter the html file</li> </ul> <pre><code>curl -sN -X POST http://localhost:8000/automlplus/web_access/analyze/ \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"file=@./sample_data/test.html\"\n</code></pre>"},{"location":"running_the_services/#test-website-accessibility-url-input","title":"Test: Website Accessibility (URL input)","text":"<ul> <li>This tests the accessibility of a file given a URL (it downloads the html/css)</li> <li>Only options here are to enter the url</li> </ul> <pre><code>curl -s -X POST http://localhost:8000/automlplus/web_access/analyze/ \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"url=https://alfie-project.eu\"\n  # Optionally add: -F \"extra_file_input=@./sample_data/wcag_guidelines.txt\"\n</code></pre>"},{"location":"running_the_services/#test-image-to-website-tool","title":"Test: Image-to-Website Tool","text":"<ul> <li>If you upload an image of a website and ask the engine to create a website of it, it will do so</li> <li>Options are the prompt and the image file</li> </ul> <pre><code>curl -sN -X POST http://localhost:8000/automlplus/image_tools/run_on_image_stream/ \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"prompt=Recreate this image into a website with HTML/CSS/JS and explain how to run it.\" \\\n  -F \"image_file=@./sample_data/websample.png\"\n</code></pre>"},{"location":"running_the_services/#test-automl-tabular","title":"Test: AutoML Tabular","text":"<ul> <li>!!!Note: This requires AutoDW to be running on the side, without this nothing will work. This test demonstrates the workflow for running a tabular AutoML job using the FastAPI backend integrated with AutoDW.</li> <li>Supports tabular datasets fetched directly from AutoDW. Requires:</li> <li>user_id (AutoDW user identifier)</li> <li>dataset_id (AutoDW dataset identifier)</li> <li>dataset_version (AutoDW dataset identifier)</li> <li>target_column_name</li> <li>task_type (classification, regression, time_series)</li> <li>time_budget in seconds</li> <li>task_id (AutoDW task identifier) [Will be deprecated eventually]</li> </ul> <p>Supported dataset formats: CSV, TSV, Parquet</p>"},{"location":"running_the_services/#trigger-automl-training-best-model-search","title":"Trigger AutoML Training + Best Model Search","text":"<p>The entire process is handled by a single endpoint: <code>POST /automl_tabular/best_model/</code></p> <p>Example cURL Command <pre><code>curl -s -X POST \"http://localhost:8001/automl_tabular/best_model/\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"user_id=101\" \\\n  -F \"dataset_id=55\" \\\n  -F \"task_id=1\" \\\n  -F \"target_column_name=signature\" \\\n  -F \"task_type=classification\" \\\n  -F \"time_stamp_column_name=\" \\\n  -F \"time_budget=30\"\n</code></pre></p>"},{"location":"running_the_services/#what-this-does","title":"What This Does","text":"<ul> <li>Fetches dataset metadata from AutoDW</li> <li>Downloads dataset file</li> <li>Validates:</li> <li>Dataset structure</li> <li>Target column</li> <li>Timestamp column (if time-series)</li> <li>Task type</li> <li>Performs AutoML training within the time budget</li> <li>Serializes and uploads:</li> <li>The best model (the entire folder generated by AutoGluon is uploaded as a zip file)</li> <li>Leaderboard as JSON + Markdown</li> <li>Successful Response Example <pre><code>{\n  \"message\": \"AutoML training completed successfully and model uploaded to AutoDW\",\n  \"leaderboard\": \"| model | score | ... |\"\n}\n</code></pre></li> </ul>"},{"location":"running_the_services/#error-handling","title":"Error Handling","text":"<ul> <li>400 \u2013 Validation Errors</li> <li>Target column missing</li> <li>Unsupported file format</li> <li>Task type invalid</li> <li>Timestamp column missing for time-series</li> <li>502 \u2013 AutoDW Communication Failure</li> <li>Metadata request fails</li> <li>File download fails</li> <li>500 \u2013 Unexpected Failures</li> <li>Training crashes</li> <li>Serialization issues</li> <li>Unexpected runtime errors</li> </ul>"},{"location":"running_the_services/#notes","title":"Notes","text":"<ul> <li>Only csv, tsv, and parquet formats are supported.</li> <li>All data is handled in a temporary directory isolated per request.</li> <li>The AutoML leaderboard is uploaded to AutoDW as both JSON and Markdown.</li> <li>The uploaded model includes metadata such as:</li> <li>model_type</li> <li>training_dataset</li> <li>framework</li> <li>description</li> </ul>"},{"location":"running_the_services/#test-automl-vision","title":"Test: AutoML Vision","text":"<ul> <li>THIS IS A WIP, AND DOES NOT WORK AS INTENDED AT THE MOMENT</li> <li>AutoML for vision</li> <li>Train/test/val split will be automatically done</li> <li>The first call for each is to interface with AutoDW (When this is possible)</li> <li> <p>The required options are the csv file with labels, a zip of images, the filename column, label column name, task type, time budget and model size</p> <ul> <li> <p>image_zip</p> <ul> <li>Images in a VERY specific format (for now, probably will be replaced with croissant or similar format later</li> <li>main folder<ul> <li>category1<ul> <li>image1.png (images of type jpg, png, jpeg)</li> <li>image2.png </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>csv_file should contain two columns - one with the file name (no path, just the file name), and      |filename|label|     |---|---|     |image1.png|cat|     |image2.png|dog|</p> </li> <li>time budget in seconds</li> <li>model size is either small,medium or large (based on number of parameters)<ul> <li>MODEL_SMALL_MAX_PARAM_SIZE=50000000</li> <li>MODEL_MEDIUM_MAX_PARAM_SIZE=200000000</li> </ul> </li> </ul> </li> </ul>"},{"location":"running_the_services/#step-1-start-a-session","title":"Step 1: Start a session","text":"<pre><code>curl -s -X POST http://localhost:8002/automl_vision/get_user_input/ \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"csv_file=@./sample_data/Garbage_Dataset_Classification/metadata.csv\" \\\n  -F \"images_zip=@./sample_data/Garbage_Dataset_Classification/images.zip\" \\\n  -F \"filename_column=filename\" \\\n  -F \"label_column=label\" \\\n  -F \"task_type=classification\" \\\n  -F \"time_budget=10\" \\\n  -F \"model_size=medium\"\n</code></pre> <p>This returns a JSON with a <code>session_id</code>.</p>"},{"location":"running_the_services/#step-2-train-and-find-best-model","title":"Step 2: Train and find best model","text":"<pre><code>curl -s -X POST http://localhost:8002/automl_vision/find_best_model/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"session_id\": \"REPLACE_WITH_SESSION_ID\"}'\n</code></pre>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#run-all-tests","title":"Run all tests","text":"<pre><code>pytest -q\n</code></pre>"},{"location":"testing/#run-fast-subset-default-vs-full-suite","title":"Run fast subset (default) vs full suite","text":"<p>The test suite skips long-running tests by default. To run the full suite: <pre><code>pytest -q --full\n</code></pre></p>"},{"location":"testing/#run-specific-testsfiles","title":"Run specific tests/files","text":"<pre><code>pytest -q tests/tabular_automl/test_modules.py::test_trainer_init\npytest -q tests/tabular_automl/test_services.py\n</code></pre>"},{"location":"testing/#notes","title":"Notes","text":"<ul> <li>Tests isolate filesystem operations using <code>tmp_path</code> and mock/monkeypatch database sessions to an in-memory SQLite engine.</li> <li>HTTP endpoint tests use <code>fastapi.testclient.TestClient</code> and monkeypatch service-layer functions to avoid heavy training runs.</li> </ul>"},{"location":"testing/#development","title":"Development","text":"<ul> <li>Useful scripts:</li> <li><code>mk_sample_data.sh</code>: helpers for sample data.</li> <li><code>test_services.sh</code>: quick test runner.</li> </ul>"},{"location":"api/automlplus/","title":"Website Accessibility","text":"<p>FastAPI endpoints for website accessibility analysis and chat utilities.</p>"},{"location":"api/automlplus/#app.automlplus.main.analyze_web_accessibility_and_readability","title":"<code>analyze_web_accessibility_and_readability(file, url=None, extra_file_input=None)</code>  <code>async</code>","text":"<p>Run WCAG-inspired accessibility checks and optional readability analysis on HTML.</p> Source code in <code>app/automlplus/main.py</code> <pre><code>@app.post(\"/automlplus/web_access/analyze/\")\nasync def analyze_web_accessibility_and_readability(\n    file: Annotated[UploadFile, File(..., description=\"HTML file\")],\n    url: Annotated[str | None, Form(..., description=\"URL of website\")] = None,\n    extra_file_input: Annotated[\n        UploadFile | None, File(..., description=\"Extra file for LLM context\")\n    ] = None,\n) -&gt; JSONResponse:\n    \"\"\"Run WCAG-inspired accessibility checks and optional readability analysis on HTML.\"\"\"\n    logger.info(\"Starting web accessibility and readability analysis\")\n\n    content: str | None = None\n    source_name: str = \"uploaded.html\"\n    timeout: int = int(os.getenv(\"WEB_ACCESSIBILITY_URL_RETRY_TIMEOUT\", 10))\n\n    # --- LoaOd HTML content ---\n    if file:\n        try:\n            content = (await file.read()).decode(\"utf-8\", errors=\"replace\")\n            source_name = file.filename or source_name\n            logger.debug(f\"HTML file '{source_name}' successfully loaded\")\n        finally:\n            try:\n                await file.close()\n            except Exception:\n                logger.warning(\"Failed to close uploaded HTML file\", exc_info=True)\n\n    if url:\n        try:\n            logger.debug(f\"Fetching HTML from URL: {url}\")\n            resp = requests.get(url, timeout=timeout)\n            resp.raise_for_status()\n            content = resp.text\n            source_name = url\n            logger.debug(\"HTML successfully fetched from URL\")\n        except Exception as e:\n            logger.error(f\"Failed to fetch HTML from URL: {e}\")\n            return JSONResponse(\n                content={\"error\": f\"Failed to fetch URL: {e}\"}, status_code=400\n            )\n\n    if not content or not str(content).strip():\n        logger.error(\"Resolved HTML content is empty\")\n        return JSONResponse(\n            content={\"error\": \"Resolved content is empty\"}, status_code=400\n        )\n\n    content_str: str = str(content)\n\n    # --- Load guidelines file if provided ---\n    context_str: str = \"\"\n    if extra_file_input is not None:\n        try:\n            logger.debug(\"Reading extra context file for accessibility analysis\")\n            guidelines_bytes = await extra_file_input.read()\n            guidelines_text = guidelines_bytes.decode(\"utf-8\", errors=\"replace\")\n            context_str = f\"Accessibility guidelines to follow (user-provided):\\n\\n{guidelines_text}\"\n            logger.debug(\"Extra context file successfully loaded\")\n        finally:\n            try:\n                await extra_file_input.close()\n            except Exception:\n                logger.warning(\"Failed to close extra context file\", exc_info=True)\n\n    # --- Run accessibility pipeline ---\n    chunk_size: int = int(os.getenv(\"CHUNK_SIZE_FOR_ACCESSIBILITY\", 3000))\n    concurrency_num: int = int(os.getenv(\"CONCURRENCY_NUM_FOR_ACCESSIBILITY\", 4))\n    logger.debug(\n        f\"Running accessibility pipeline with chunk size {chunk_size}, concurrency {concurrency_num}\"\n    )\n\n    results = await run_accessibility_pipeline(\n        content=content_str,\n        filename=source_name,\n        jinja_environment=jinja_environment,\n        chunk_size=chunk_size,\n        concurrency=concurrency_num,\n        context=context_str,\n    )\n    logger.info(\"Accessibility pipeline completed successfully\")\n\n    # --- Aggregate results ---\n    resolved_results = [await resolve_coroutines(r) for r in results]\n\n    scores = [\n        r.get(\"score\")\n        for r in resolved_results\n        if isinstance(r.get(\"score\"), (int, float))\n    ]\n    average_score: float | None = (sum(scores) / len(scores)) if scores else None\n    logger.debug(f\"Computed average accessibility score: {average_score}\")\n\n    # --- Readability analysis ---\n    readability_scores: dict[str, Any] | None = None\n    try:\n        text = extract_text_from_html_bytes(content_str.encode(\"utf-8\"))\n        if text.strip():\n            readability_scores = ReadabilityAnalyzer.analyze(text)\n            logger.debug(\"Readability analysis completed successfully\")\n    except Exception as e:\n        logger.warning(f\"Error during readability analysis: {e}\")\n        readability_scores = {\"error\": str(e)}\n\n    payload = {\n        \"source\": source_name,\n        \"average_score\": average_score,\n        \"results\": resolved_results,\n        \"readability\": readability_scores,\n    }\n\n    safe_payload = json_safe(payload)\n    logger.info(\"Web accessibility and readability analysis finished successfully\")\n    return JSONResponse(content=safe_payload)\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.main.check_alt_text","title":"<code>check_alt_text(image_url=Form(...), alt_text=Form(...))</code>  <code>async</code>","text":"<p>Evaluate provided alt text against the referenced image using an LLM.</p> Source code in <code>app/automlplus/main.py</code> <pre><code>@app.post(\"/automlplus/web_access/check-alt-text/\")\nasync def check_alt_text(\n    image_url: str = Form(...),\n    alt_text: str = Form(...),\n) -&gt; JSONResponse:\n    \"\"\"Evaluate provided alt text against the referenced image using an LLM.\"\"\"\n    logger.info(f\"Checking alt text for image URL: {image_url}\")\n    try:\n        result: dict[str, Any] = AltTextChecker.check(\n            jinja_environment, image_url, alt_text\n        )\n        logger.info(\"Alt-text evaluation completed successfully\")\n\n        safe_result = json_safe(\n            {\n                \"src\": image_url,\n                \"alt_text\": alt_text,\n                \"evaluation\": result,\n            }\n        )\n        return JSONResponse(content=safe_result)\n    except Exception as e:\n        logger.exception(\"Error during alt-text check: %s\", e)\n        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.main.image_to_website","title":"<code>image_to_website(image_file=File(default=None))</code>  <code>async</code>","text":"<p>Convert an uploaded image into a basic HTML website structure.</p> Source code in <code>app/automlplus/main.py</code> <pre><code>@app.post(\"/automlplus/image_tools/image_to_website/\")\nasync def image_to_website(\n    image_file: UploadFile | None = File(default=None),\n) -&gt; JSONResponse:\n    \"\"\"Convert an uploaded image into a basic HTML website structure.\"\"\"\n    logger.info(\"Starting image-to-website conversion\")\n    try:\n        # TODO: Implement image-to-website logic (currently placeholder)\n        logger.info(\"Image-to-website conversion completed successfully\")\n        return JSONResponse(content={})\n    except Exception as e:\n        logger.exception(\"Error during image-to-website conversion: %s\", e)\n        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.main.json_safe","title":"<code>json_safe(data)</code>","text":"<p>Recursively convert all string values in a dict or list to JSON-safe strings (escape quotes, line breaks, etc.) so JSONResponse won't fail.</p> Source code in <code>app/automlplus/main.py</code> <pre><code>def json_safe(data: Any) -&gt; Any:\n    \"\"\"\n    Recursively convert all string values in a dict or list to JSON-safe strings\n    (escape quotes, line breaks, etc.) so JSONResponse won't fail.\n    \"\"\"\n    if isinstance(data, dict):\n        logger.debug(\"Processing dict for JSON safety\")\n        return {k: json_safe(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        logger.debug(\"Processing list for JSON safety\")\n        return [json_safe(v) for v in data]\n    elif isinstance(data, str):\n        logger.debug(\"Escaping special characters in string for JSON safety\")\n        return (\n            data.replace(\"\\\\\", \"\\\\\\\\\")\n            .replace('\"', '\\\\\"')\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"\\r\", \"\\\\r\")\n        )\n    else:\n        return data\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.main.run_on_image","title":"<code>run_on_image(prompt=Form(...), model=Form(default=None), image_file=File(default=None), image_url=Form(default=None))</code>  <code>async</code>","text":"<p>Run a vision-language model on an image and return the text output.</p> Source code in <code>app/automlplus/main.py</code> <pre><code>@app.post(\"/automlplus/image_tools/run_on_image/\")\nasync def run_on_image(\n    prompt: str = Form(...),\n    model: str | None = Form(default=None),\n    image_file: UploadFile | None = File(default=None),\n    image_url: str | None = Form(default=None),\n) -&gt; JSONResponse:\n    \"\"\"Run a vision-language model on an image and return the text output.\"\"\"\n    logger.info(\"Running model on image with prompt: %s\", prompt)\n\n    if image_file is None and not image_url:\n        logger.error(\"Missing both image_file and image_url\")\n        return JSONResponse(\n            {\"error\": \"Provide image_file or image_url\"}, status_code=400\n        )\n\n    try:\n        image_bytes: bytes | None = await image_file.read() if image_file else None\n        if image_file:\n            await image_file.close()\n            logger.debug(\"Image file successfully read and closed\")\n\n        result = ImagePromptRunner.run(\n            image_bytes=image_bytes,\n            image_path_or_url=image_url,\n            prompt=prompt,\n            model=model,\n            jinja_environment=jinja_environment,\n        )\n\n        safe_result = json_safe({\"response\": result})\n        logger.info(\"Image prompt run completed successfully\")\n        return JSONResponse(content=safe_result)\n    except Exception as e:\n        logger.exception(\"Error during image prompt run: %s\", e)\n        return JSONResponse({\"error\": str(e)}, status_code=500)\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.main.run_on_image_stream","title":"<code>run_on_image_stream(prompt='', model=None, image_file=None, image_url=None)</code>  <code>async</code>","text":"<p>Stream a vision-language model's output on an image and prompt.</p> Source code in <code>app/automlplus/main.py</code> <pre><code>@app.post(\n    \"/automlplus/image_tools/run_on_image_stream/\",\n    response_model=None,\n)\nasync def run_on_image_stream(\n    prompt: Annotated[str, Form(..., description=\"Prompt to apply on the image\")] = \"\",\n    model: Annotated[\n        str | None, Form(..., description=\"Model to apply on the image\")\n    ] = None,\n    image_file: Annotated[\n        UploadFile | None, File(..., description=\"Image file if not a URL\")\n    ] = None,\n    image_url: Annotated[\n        str | None, Form(..., description=\"Image URL if not a file but an URL\")\n    ] = None,\n) -&gt; Response:\n    \"\"\"Stream a vision-language model's output on an image and prompt.\"\"\"\n    logger.info(\"Streaming model output for image prompt: %s\", prompt)\n\n    if image_file is None and not image_url:\n        logger.error(\"No image or URL provided for streaming run\")\n        return JSONResponse(\n            content={\"error\": \"Provide image_file or image_url\"}, status_code=400\n        )\n\n    try:\n        image_bytes: bytes | None = None\n        if image_file is not None:\n            try:\n                image_bytes = await image_file.read()\n                logger.debug(\"Image file successfully read for streaming\")\n            finally:\n                try:\n                    await image_file.close()\n                except Exception:\n                    logger.warning(\"Failed to properly close image file\", exc_info=True)\n\n        def generator():\n            logger.debug(\"Starting stream generator for image model run\")\n            for chunk in ImagePromptRunner.run_stream(\n                image_bytes=image_bytes,\n                image_path_or_url=image_url,\n                prompt=prompt,\n                model=model,\n                jinja_environment=jinja_environment,\n            ):\n                yield chunk\n\n        logger.info(\"Image stream initiated successfully\")\n        return StreamingResponse(generator(), media_type=\"text/plain\")\n\n    except Exception as e:\n        logger.exception(\"Error during image prompt streaming run: %s\", e)\n        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.utils.ImageConverter","title":"<code>ImageConverter</code>","text":"<p>Convert images to base64 from local paths or URLs.</p> Source code in <code>app/automlplus/utils.py</code> <pre><code>class ImageConverter:\n    \"\"\"Convert images to base64 from local paths or URLs.\"\"\"\n\n    @staticmethod\n    def to_base64(image_path_or_url: str) -&gt; str:\n        logger.info(\"Converting image to base64: %s\", image_path_or_url)\n        try:\n            if image_path_or_url.startswith(\"http\"):\n                headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; ImageConverter/1.0)\"}\n                resp = requests.get(image_path_or_url, headers=headers)\n                resp.raise_for_status()\n                if \"image\" not in resp.headers.get(\"Content-Type\", \"\"):\n                    raise ValueError(\n                        f\"URL does not point to an image: {image_path_or_url}\"\n                    )\n                image = Image.open(BytesIO(resp.content))\n            else:\n                if not os.path.isfile(image_path_or_url):\n                    raise FileNotFoundError(f\"No such file: {image_path_or_url}\")\n                image = Image.open(image_path_or_url)\n\n            image = image.convert(\"RGBA\")\n            buffer = BytesIO()\n            image.save(buffer, format=\"PNG\")\n            return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        except Exception as e:\n            logger.exception(\"Image conversion failed\")\n            raise e\n\n    @staticmethod\n    def bytes_to_base64(image_bytes: bytes) -&gt; str:\n        \"\"\"Convert raw image bytes to base64 PNG string.\"\"\"\n        try:\n            image = Image.open(BytesIO(image_bytes))\n            buffer = BytesIO()\n            image.save(buffer, format=\"PNG\")\n            return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        except Exception as e:\n            logger.exception(\"Image bytes conversion failed\")\n            raise e\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.utils.ImageConverter.bytes_to_base64","title":"<code>bytes_to_base64(image_bytes)</code>  <code>staticmethod</code>","text":"<p>Convert raw image bytes to base64 PNG string.</p> Source code in <code>app/automlplus/utils.py</code> <pre><code>@staticmethod\ndef bytes_to_base64(image_bytes: bytes) -&gt; str:\n    \"\"\"Convert raw image bytes to base64 PNG string.\"\"\"\n    try:\n        image = Image.open(BytesIO(image_bytes))\n        buffer = BytesIO()\n        image.save(buffer, format=\"PNG\")\n        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n    except Exception as e:\n        logger.exception(\"Image bytes conversion failed\")\n        raise e\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.imagetools.ImagePromptRunner","title":"<code>ImagePromptRunner</code>","text":"<p>Run a VLM on an image and user-provided prompt.</p> Source code in <code>app/automlplus/imagetools.py</code> <pre><code>class ImagePromptRunner:\n    \"\"\"Run a VLM on an image and user-provided prompt.\"\"\"\n\n    DEFAULT_MODEL: str = os.getenv(\"IMAGE_PROMPT_MODEL\", \"qwen2.5vl\")\n\n    @staticmethod\n    def _resolve_model(model: str | None) -&gt; str:\n        if not model or not str(model).strip():\n            return ImagePromptRunner.DEFAULT_MODEL\n        return model\n\n    @staticmethod\n    def build_messages(\n        jinja_environment: Environment | None, image_b64: str, prompt: str\n    ) -&gt; list[dict[str, str | list[str] | list[None]]]:\n        # If a jinja environment is provided, try to render a default system prompt; otherwise minimal messages\n        messages: list[dict[str, str | list[str] | list[None]]] = []\n        if jinja_environment is not None:\n            try:\n                system_prompt = render_template(\n                    jinja_environment, \"image_to_website_prompt.txt\"\n                )\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            except Exception as e:\n                raise e\n        messages.append({\"role\": \"user\", \"content\": prompt, \"images\": [image_b64]})\n        return messages\n\n    @staticmethod\n    def run(\n        image_bytes: bytes | None = None,\n        image_path_or_url: str | None = None,\n        prompt: str = \"\",\n        model: str | None = None,\n        jinja_environment: Environment | None = None,\n    ) -&gt; str:\n        model_name = ImagePromptRunner._resolve_model(model)\n        try:\n            if image_bytes is None and not image_path_or_url:\n                raise ValueError(\"Provide either image_bytes or image_path_or_url\")\n\n            image_b64 = (\n                ImageConverter.bytes_to_base64(image_bytes)\n                if image_bytes is not None\n                else ImageConverter.to_base64(str(image_path_or_url))\n            )\n\n            messages = ImagePromptRunner.build_messages(\n                jinja_environment, image_b64, prompt\n            )\n\n            # Use central ChatHandler for synchronous message-based chat (supports images)\n            return ChatHandler.chat_sync_messages(messages=messages, model=model_name)\n        except Exception as e:\n            logger.exception(\"ImagePromptRunner failed\")\n            raise e\n\n    @staticmethod\n    def run_stream(\n        image_bytes: bytes | None = None,\n        image_path_or_url: str | None = None,\n        prompt: str = \"\",\n        model: str | None = None,\n        jinja_environment: Environment | None = None,\n    ) -&gt; str:\n        \"\"\"Stream VLM output for an image+prompt interaction.\n\n        Yields incremental text chunks.\n        \"\"\"\n        model_name = ImagePromptRunner._resolve_model(model)\n        if image_bytes is None and not image_path_or_url:\n            raise ValueError(\"Provide either image_bytes or image_path_or_url\")\n\n        image_b64 = (\n            ImageConverter.bytes_to_base64(image_bytes)\n            if image_bytes is not None\n            else ImageConverter.to_base64(str(image_path_or_url))\n        )\n        messages = ImagePromptRunner.build_messages(\n            jinja_environment, image_b64, prompt\n        )\n        return ChatHandler.chat_stream_messages_sync(\n            messages=messages, model=model_name\n        )\n</code></pre>"},{"location":"api/automlplus/#app.automlplus.imagetools.ImagePromptRunner.run_stream","title":"<code>run_stream(image_bytes=None, image_path_or_url=None, prompt='', model=None, jinja_environment=None)</code>  <code>staticmethod</code>","text":"<p>Stream VLM output for an image+prompt interaction.</p> <p>Yields incremental text chunks.</p> Source code in <code>app/automlplus/imagetools.py</code> <pre><code>@staticmethod\ndef run_stream(\n    image_bytes: bytes | None = None,\n    image_path_or_url: str | None = None,\n    prompt: str = \"\",\n    model: str | None = None,\n    jinja_environment: Environment | None = None,\n) -&gt; str:\n    \"\"\"Stream VLM output for an image+prompt interaction.\n\n    Yields incremental text chunks.\n    \"\"\"\n    model_name = ImagePromptRunner._resolve_model(model)\n    if image_bytes is None and not image_path_or_url:\n        raise ValueError(\"Provide either image_bytes or image_path_or_url\")\n\n    image_b64 = (\n        ImageConverter.bytes_to_base64(image_bytes)\n        if image_bytes is not None\n        else ImageConverter.to_base64(str(image_path_or_url))\n    )\n    messages = ImagePromptRunner.build_messages(\n        jinja_environment, image_b64, prompt\n    )\n    return ChatHandler.chat_stream_messages_sync(\n        messages=messages, model=model_name\n    )\n</code></pre>"},{"location":"api/core/","title":"Core","text":"<p>Chat handling utilities for async LLM requests.</p> <p>Provides a simple queued interface (<code>ChatQueue</code>) and a static facade (<code>ChatHandler</code>) to interact with local Ollama models, supporting both regular and streaming responses.</p>"},{"location":"api/core/#app.core.chat_handler.ChatHandler","title":"<code>ChatHandler</code>","text":"Source code in <code>app/core/chat_handler.py</code> <pre><code>class ChatHandler:\n    queue = ChatQueue()\n\n    @staticmethod\n    async def init():\n        logger.debug(f\"Started queue\")\n        await ChatHandler.queue.start()\n\n    @staticmethod\n    async def chat(\n        message, context=\"\", backend=\"ollama\", model=\"gemma3:4b\", stream=False\n    ):\n        return await ChatHandler.queue.submit(message, context, backend, model, stream)\n\n    @staticmethod\n    async def dispatch(message, context, backend, model):\n        logger.debug(f\"Dispatch Chat to backend {backend} with {message}, {context}\")\n        \"\"\"Route chat requests to the correct backend.\"\"\"\n        if backend.lower() == \"ollama\":\n            return await ChatHandler._ollama_chat(message, context, model)\n        elif backend.lower() == \"azure\":\n            return await ChatHandler._azure_chat(message, context, model)\n        else:\n            raise ValueError(f\"Unknown chat backend: {backend}\")\n\n    @staticmethod\n    async def dispatch_stream(message, context, backend, model):\n        logger.debug(\n            f\"Dispatch Chat Stream to backend {backend} with {message}, {context}\"\n        )\n        if backend.lower() == \"ollama\":\n            async for chunk in ChatHandler._ollama_chat_stream(message, context, model):\n                yield chunk\n        elif backend.lower() == \"azure\":\n            async for chunk in ChatHandler._azure_chat_stream(message, context, model):\n                yield chunk\n        else:\n            raise ValueError(f\"Unknown chat backend: {backend}\")\n\n    @staticmethod\n    async def _ollama_chat(message, context, model):\n        from ollama import Client\n\n        chat = Client(timeout=120).chat\n        logger.debug(f\"Ollama client init\")\n\n        response = chat(\n            model=model,\n            messages=[\n                {\"role\": \"user\", \"content\": message},\n                {\"role\": \"user-hidden\", \"content\": context},\n            ],\n        )\n        return response[\"message\"][\"content\"].strip()\n\n    @staticmethod\n    async def _ollama_chat_stream(message, context, model):\n        from ollama import Client\n\n        chat = Client(timeout=120).chat\n        logger.debug(f\"Ollama client init stream\")\n        stream = chat(\n            model=model,\n            messages=[\n                {\"role\": \"user\", \"content\": message},\n                {\"role\": \"user-hidden\", \"content\": context},\n            ],\n            stream=True,\n        )\n        async for chunk in stream:\n            content = chunk.get(\"message\", {}).get(\"content\", \"\")\n            if content:\n                yield content\n\n    # --- Synchronous helpers for structured message payloads (incl. images) ---\n    @staticmethod\n    def chat_sync_messages(\n        messages: List[dict], backend: str = \"ollama\", model: str = \"gemma3:4b\"\n    ) -&gt; str:\n        \"\"\"Synchronously send a list of chat messages (optionally with images) to a backend.\n\n        This is useful for callers that aren't async and need to pass through\n        full message structures, e.g., for VLM prompts that include an\n        \"images\" field supported by Ollama.\n        \"\"\"\n        logger.debug(f\"Dispatch synchronous Chat to backend {backend} with {messages}\")\n        backend_lower = backend.lower()\n        if backend_lower == \"ollama\":\n            return ChatHandler._ollama_chat_messages_sync(messages, model)\n        elif backend_lower == \"azure\":\n            return ChatHandler._azure_chat_messages_sync(messages, model)\n        else:\n            raise ValueError(f\"Unknown chat backend: {backend}\")\n\n    @staticmethod\n    def _ollama_chat_messages_sync(messages: List[dict], model: str) -&gt; str:\n        from ollama import Client\n\n        chat = Client(timeout=300).chat\n        logger.debug(f\"Ollama client cht synchronous init\")\n        response = chat(\n            model=model,\n            messages=messages,\n        )\n        return response[\"message\"][\"content\"].strip()\n\n    # --- Streaming helpers for structured message payloads (incl. images) ---\n    @staticmethod\n    def chat_stream_messages_sync(\n        messages: List[dict], backend: str = \"ollama\", model: str = \"gemma3:4b\"\n    ):\n        \"\"\"Synchronously stream a list of chat messages (optionally with images).\n\n        Yields incremental text chunks from the backend as they arrive.\n        \"\"\"\n        logger.debug(f\"Stream chat messages sychronously\")\n        backend_lower = backend.lower()\n        if backend_lower == \"ollama\":\n            return ChatHandler._ollama_chat_messages_stream_sync(messages, model)\n        elif backend_lower == \"azure\":\n            raise NotImplementedError(\n                \"Azure chat (messages stream) not implemented yet.\"\n            )\n        else:\n            raise ValueError(f\"Unknown chat backend: {backend}\")\n\n    @staticmethod\n    def _ollama_chat_messages_stream_sync(messages: List[dict], model: str):\n        from ollama import Client\n\n        chat = Client(timeout=300).chat\n        stream = chat(\n            model=model,\n            messages=messages,\n            stream=True,\n        )\n        logger.debug(f\"Ollama client stream in chunks\")\n        for chunk in stream:\n            content = (chunk or {}).get(\"message\", {}).get(\"content\", \"\")\n            if content:\n                yield content\n\n    @staticmethod\n    def _azure_chat_messages_sync(messages: List[dict], model: str) -&gt; str:\n        \"\"\"Synchronous Azure chat that supports text and images.\n\n        Accepts our internal message dicts of the form:\n          {\"role\": \"system\"|\"user\", \"content\": str | None, \"images\": [base64str, ...]?}\n\n        Converts them into Azure AI Inference message objects. For messages that\n        include images, constructs a single UserMessage with a mixed content list\n        containing a text item (when provided) and one input_image item per image.\n        \"\"\"\n        client = ChatHandler._get_azure_client()\n        logger.debug(f\"Azure client init stream\")\n\n        def to_azure_messages(msgs: List[dict]):\n            azure_messages: List[object] = []\n\n            for m in msgs:\n                role = (m.get(\"role\") or \"user\").lower()\n                text_content = m.get(\"content\")\n                images = m.get(\"images\") or []\n\n                # If no images, fall back to simple text message\n                if not images:\n                    if role == \"system\":\n                        azure_messages.append(SystemMessage(content=text_content or \"\"))\n                    else:\n                        azure_messages.append(UserMessage(content=text_content or \"\"))\n                    continue\n\n                # For image-bearing messages, build a multi-part content list.\n                # Using raw dicts for compatibility across SDK variants.\n                mixed_content: List[object] = []\n                if text_content:\n                    mixed_content.append({\"type\": \"text\", \"text\": text_content})\n                for b64 in images:\n                    if not isinstance(b64, str) or not b64:\n                        continue\n                    mixed_content.append(\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"},\n                        }\n                    )\n\n                if role == \"system\":\n                    # Azure doesn't accept images in system role; downgrade to user\n                    azure_messages.append(UserMessage(content=mixed_content))\n                else:\n                    azure_messages.append(UserMessage(content=mixed_content))\n\n            return azure_messages\n\n        azure_msgs = to_azure_messages(messages)\n        logger.debug(f\"Azure message dict {azure_msgs}\")\n        response = client.complete(model=model, messages=azure_msgs)\n        return ChatHandler._extract_azure_text_from_response(response)\n\n    @staticmethod\n    def _get_azure_client():\n        \"\"\"Initialize Azure AI Foundry chat client.\"\"\"\n        endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_LARGE_MODEL\")\n        api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n        if not endpoint or not api_key:\n            raise RuntimeError(\n                \"Missing AZURE_OPENAI_ENDPOINT_LARGE_MODEL or AZURE_OPENAI_KEY environment variables\"\n            )\n        logger.debug(f\"Endpoint and API Key Exsists\")\n        return ChatCompletionsClient(\n            endpoint=endpoint, credential=AzureKeyCredential(api_key)\n        )\n\n    @staticmethod\n    async def _azure_chat(message, context, model):\n        \"\"\"Non-streaming chat call using Azure GPT-4o-mini.\"\"\"\n        client = ChatHandler._get_azure_client()\n        messages = [\n            SystemMessage(content=context or \"You are a helpful assistant.\"),\n            UserMessage(content=message),\n        ]\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, lambda: client.complete(model=model, messages=messages)\n        )\n        logger.debug(f\"Azure chat async works\")\n        return ChatHandler._extract_azure_text_from_response(response)\n\n    @staticmethod\n    async def _azure_chat_stream(message, context, model):\n        \"\"\"Streaming chat call using Azure GPT-4o-mini.\"\"\"\n        client = ChatHandler._get_azure_client()\n        messages = [\n            SystemMessage(content=context or \"You are a helpful assistant.\"),\n            UserMessage(content=message),\n        ]\n\n        loop = asyncio.get_event_loop()\n\n        def sync_stream():\n            return client.complete(\n                model=model,\n                messages=messages,\n                stream=True,\n            )\n\n        # Run the sync generator in a thread and forward chunks asynchronously\n        stream = await loop.run_in_executor(None, sync_stream)\n        for event in stream:\n            if hasattr(event, \"delta\") and event.delta:\n                delta = event.delta\n                content = getattr(delta, \"content\", None)\n                if content is None and isinstance(delta, dict):\n                    content = delta.get(\"content\")\n                # content may be str or list of items (with text)\n                if isinstance(content, str):\n                    if content:\n                        yield content\n                elif isinstance(content, list):\n                    for item in content:\n                        if isinstance(item, str):\n                            if item:\n                                yield item\n                        elif isinstance(item, dict):\n                            text = item.get(\"text\")\n                            if text:\n                                yield text\n                        else:\n                            text = getattr(item, \"text\", None)\n                            if text:\n                                yield text\n        logger.debug(f\"Azure chat stream works\")\n        if hasattr(stream, \"close\"):\n            stream.close()\n\n    @staticmethod\n    def _extract_azure_text_from_response(response) -&gt; str:\n        \"\"\"Extract text content from Azure ChatCompletions response across SDK variants.\"\"\"\n        # Try choices-based response (common structure)\n        try:\n            choice0 = response.choices[0]\n            message = getattr(choice0, \"message\", None) or choice0[\"message\"]\n            content = getattr(message, \"content\", None)\n            if content is None and isinstance(message, dict):\n                content = message.get(\"content\")\n            # content can be str or list of content parts\n            if isinstance(content, str):\n                return content.strip()\n            if isinstance(content, list):\n                parts = []\n                for part in content:\n                    if isinstance(part, str):\n                        parts.append(part)\n                    elif isinstance(part, dict):\n                        txt = part.get(\"text\")\n                        if txt:\n                            parts.append(txt)\n                    else:\n                        txt = getattr(part, \"text\", None)\n                        if txt:\n                            parts.append(txt)\n                return \"\".join(parts).strip()\n        except Exception:\n            pass\n        # Fallback older shape: output_message.content[0].text\n        try:\n            return response.output_message.content[0].text.strip()\n        except Exception:\n            pass\n        # Last resort: str(response)\n        return str(response)\n</code></pre>"},{"location":"api/core/#app.core.chat_handler.ChatHandler.chat_stream_messages_sync","title":"<code>chat_stream_messages_sync(messages, backend='ollama', model='gemma3:4b')</code>  <code>staticmethod</code>","text":"<p>Synchronously stream a list of chat messages (optionally with images).</p> <p>Yields incremental text chunks from the backend as they arrive.</p> Source code in <code>app/core/chat_handler.py</code> <pre><code>@staticmethod\ndef chat_stream_messages_sync(\n    messages: List[dict], backend: str = \"ollama\", model: str = \"gemma3:4b\"\n):\n    \"\"\"Synchronously stream a list of chat messages (optionally with images).\n\n    Yields incremental text chunks from the backend as they arrive.\n    \"\"\"\n    logger.debug(f\"Stream chat messages sychronously\")\n    backend_lower = backend.lower()\n    if backend_lower == \"ollama\":\n        return ChatHandler._ollama_chat_messages_stream_sync(messages, model)\n    elif backend_lower == \"azure\":\n        raise NotImplementedError(\n            \"Azure chat (messages stream) not implemented yet.\"\n        )\n    else:\n        raise ValueError(f\"Unknown chat backend: {backend}\")\n</code></pre>"},{"location":"api/core/#app.core.chat_handler.ChatHandler.chat_sync_messages","title":"<code>chat_sync_messages(messages, backend='ollama', model='gemma3:4b')</code>  <code>staticmethod</code>","text":"<p>Synchronously send a list of chat messages (optionally with images) to a backend.</p> <p>This is useful for callers that aren't async and need to pass through full message structures, e.g., for VLM prompts that include an \"images\" field supported by Ollama.</p> Source code in <code>app/core/chat_handler.py</code> <pre><code>@staticmethod\ndef chat_sync_messages(\n    messages: List[dict], backend: str = \"ollama\", model: str = \"gemma3:4b\"\n) -&gt; str:\n    \"\"\"Synchronously send a list of chat messages (optionally with images) to a backend.\n\n    This is useful for callers that aren't async and need to pass through\n    full message structures, e.g., for VLM prompts that include an\n    \"images\" field supported by Ollama.\n    \"\"\"\n    logger.debug(f\"Dispatch synchronous Chat to backend {backend} with {messages}\")\n    backend_lower = backend.lower()\n    if backend_lower == \"ollama\":\n        return ChatHandler._ollama_chat_messages_sync(messages, model)\n    elif backend_lower == \"azure\":\n        return ChatHandler._azure_chat_messages_sync(messages, model)\n    else:\n        raise ValueError(f\"Unknown chat backend: {backend}\")\n</code></pre>"},{"location":"api/core/#app.core.chat_handler.ChatQueue","title":"<code>ChatQueue</code>","text":"<p>Async work queue for chat requests, supporting streaming and non-streaming calls.</p> Source code in <code>app/core/chat_handler.py</code> <pre><code>class ChatQueue:\n    \"\"\"Async work queue for chat requests, supporting streaming and non-streaming calls.\"\"\"\n\n    def __init__(self, num_workers=4):\n        self.queue = asyncio.Queue()\n        self.tasks = []\n        self.num_workers = num_workers\n        self.semaphore = asyncio.Semaphore(num_workers)\n\n    async def start(self):\n        logger.debug(\"Started Chat task\")\n        self.tasks = [\n            asyncio.create_task(self.worker()) for _ in range(self.num_workers)\n        ]\n\n    async def worker(self):\n        while True:\n            fut, message, context, backend, model, stream, stream_queue = (\n                await self.queue.get()\n            )\n            try:\n                if stream:\n                    async for chunk in ChatHandler.dispatch_stream(\n                        message, context, backend, model\n                    ):\n                        await stream_queue.put(chunk)\n                    await stream_queue.put(None)\n                    fut.set_result(True)\n                else:\n                    result = await ChatHandler.dispatch(\n                        message, context, backend, model\n                    )\n                    fut.set_result(result)\n            except Exception as e:\n                if stream:\n                    await stream_queue.put(f\"[ERROR] {str(e)}\")\n                    await stream_queue.put(None)\n                fut.set_exception(e)\n            finally:\n                self.queue.task_done()\n\n    async def submit(\n        self, message, context=\"\", backend=\"ollama\", model=\"gemma3:4b\", stream=False\n    ):\n        async with self.semaphore:\n\n            logger.debug(\n                f\"Submitted {message} message with backend {backend}, model {model}, stream {stream}\"\n            )\n            if stream:\n\n                async def stream_gen():\n                    async for chunk in ChatHandler.dispatch_stream(\n                        message, context, backend, model\n                    ):\n                        yield chunk\n\n                return stream_gen()\n            else:\n                return await ChatHandler.dispatch(message, context, backend, model)\n</code></pre>"},{"location":"api/core/#app.core.utils.render_template","title":"<code>render_template(jinja_environment, template_name, **kwargs)</code>","text":"<p>Renders a Jinja2 template with the given arguments.</p> Source code in <code>app/core/utils.py</code> <pre><code>def render_template(\n    jinja_environment: Environment, template_name: str, **kwargs\n) -&gt; str:\n    \"\"\"\n    Renders a Jinja2 template with the given arguments.\n    \"\"\"\n    template = jinja_environment.get_template(template_name)\n    try:\n        return template.render(**kwargs)\n    except Exception as e:\n        logger.error(\"Template render failed: %s\", str(e))\n        return \"\"\n</code></pre>"},{"location":"api/tabular_automl/","title":"Tabular AutoML","text":"<p>FastAPI endpoints for tabular AutoML workflows.</p> <p>Provides endpoints to accept user data/config, validate inputs, store session metadata, and trigger AutoML training using AutoGluon.</p> <p>Database models and engine setup for tabular AutoML sessions.</p>"},{"location":"api/tabular_automl/#app.tabular_automl.main.find_best_model_for_mvp","title":"<code>find_best_model_for_mvp(request, user_id, dataset_id, dataset_version=None, target_column_name='', time_stamp_column_name=None, task_type='classification', time_budget=10)</code>  <code>async</code>","text":"<p>Fetch dataset metadata and file from AutoDW, validate it, and run AutoML training to find and upload the best model. Train and upload an AutoML model for a given tabular dataset retrieved from AutoDW.</p> This endpoint <ol> <li>Fetches dataset metadata and file from AutoDW using the provided user and dataset IDs.</li> <li>Validates dataset integrity and user-specified parameters such as target column,    timestamp column (if applicable), and task type.</li> <li>Trains an AutoML model on the dataset within a specified time budget.</li> <li>Serializes and uploads the best-performing model and leaderboard results back to AutoDW.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique user identifier from AutoDW.</p> required <code>dataset_id</code> <code>str</code> <p>Unique dataset identifier from AutoDW.</p> required <code>dataset_version</code> <code>str</code> <p>Unique dataset version from AutoDW.</p> <code>None</code> <code>target_column_name</code> <code>str</code> <p>Name of the target column in the dataset.</p> <code>''</code> <code>time_stamp_column_name</code> <code>str | None</code> <p>Name of the timestamp column for time-series tasks.</p> <code>None</code> <code>task_type</code> <code>str</code> <p>Type of ML task. One of {\"classification\", \"regression\", \"time_series\"}.</p> <code>'classification'</code> <code>time_budget</code> <code>int</code> <p>Time budget for AutoML training, in seconds.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>JSONResponse</code> <code>JSONResponse</code> <p>A structured response indicating success or failure. - On success (200): Returns a success message and leaderboard summary. - On validation error (400): Returns an error message describing the invalid input. - On AutoDW communication failure (502): Returns an error indicating network issues. - On unexpected failure (500): Returns a general error description.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If AutoDW metadata or dataset requests fail.</p> <code>Exception</code> <p>For unexpected runtime or training-related errors.</p> Example <p>HTTP POST /automl_tabular/best_model/ Form data:     user_id=101     dataset_id=55     target_column_name=\"label\"     task_type=\"classification\"     time_budget=600</p> Notes <ul> <li>Only \"csv\", \"tsv\", and \"parquet\" dataset formats are supported.</li> <li>A temporary directory is used for dataset download, model training,   and serialization before upload.</li> <li>The leaderboard is returned both as a markdown string and as JSON   when uploaded to AutoDW.</li> </ul> Source code in <code>app/tabular_automl/main.py</code> <pre><code>@app.post(\"/automl_tabular/best_model/\")\nasync def find_best_model_for_mvp(\n    request: Request,\n    user_id: Annotated[str, Form(..., description=\"User id from AutoDW\")],\n    dataset_id: Annotated[str, Form(..., description=\"User id from AutoDW\")],\n    dataset_version: Annotated[\n        str | None,\n        Form(..., description=\"Optional dataset version selection from AutoDW\"),\n    ] = None,\n    target_column_name: Annotated[\n        str, Form(..., description=\"Name of the target column\")\n    ] = \"\",\n    time_stamp_column_name: Annotated[\n        str | None,\n        Form(..., description=\"Timestamp column (required for time-series tasks)\"),\n    ] = None,\n    task_type: Annotated[\n        str,\n        Form(\n            ...,\n            description=\"Type of ML task\",\n            examples=[\"classification\", \"regression\", \"time_series\"],\n        ),\n    ] = \"classification\",\n    time_budget: Annotated[int, Form(..., description=\"Time budget in seconds\")] = 10,\n    # Task ID will be eventually deprecated. Currently, it is sent to the AutoDW\n    # when creating model because AutoDW then sends Kafka task complete message.\n) -&gt; JSONResponse:\n    \"\"\"\n    Fetch dataset metadata and file from AutoDW, validate it,\n    and run AutoML training to find and upload the best model.\n    Train and upload an AutoML model for a given tabular dataset retrieved from AutoDW.\n\n    This endpoint:\n      1. Fetches dataset metadata and file from AutoDW using the provided user and dataset IDs.\n      2. Validates dataset integrity and user-specified parameters such as target column,\n         timestamp column (if applicable), and task type.\n      3. Trains an AutoML model on the dataset within a specified time budget.\n      4. Serializes and uploads the best-performing model and leaderboard results back to AutoDW.\n\n    Args:\n        user_id (str): Unique user identifier from AutoDW.\n        dataset_id (str): Unique dataset identifier from AutoDW.\n        dataset_version (str): Unique dataset version from AutoDW.\n        target_column_name (str): Name of the target column in the dataset.\n        time_stamp_column_name (str | None): Name of the timestamp column for time-series tasks.\n        task_type (str): Type of ML task. One of {\"classification\", \"regression\", \"time_series\"}.\n        time_budget (int): Time budget for AutoML training, in seconds.\n\n    Returns:\n        JSONResponse: A structured response indicating success or failure.\n            - On success (200): Returns a success message and leaderboard summary.\n            - On validation error (400): Returns an error message describing the invalid input.\n            - On AutoDW communication failure (502): Returns an error indicating network issues.\n            - On unexpected failure (500): Returns a general error description.\n\n    Raises:\n        requests.RequestException: If AutoDW metadata or dataset requests fail.\n        Exception: For unexpected runtime or training-related errors.\n\n    Example:\n        HTTP POST /automl_tabular/best_model/\n        Form data:\n            user_id=101\n            dataset_id=55\n            target_column_name=\"label\"\n            task_type=\"classification\"\n            time_budget=600\n\n    Notes:\n        - Only \"csv\", \"tsv\", and \"parquet\" dataset formats are supported.\n        - A temporary directory is used for dataset download, model training,\n          and serialization before upload.\n        - The leaderboard is returned both as a markdown string and as JSON\n          when uploaded to AutoDW.\n    \"\"\"\n\n    autodw_base = autodw_url\n    metadata_url = f\"{autodw_base}/datasets/{user_id}/{dataset_id}\"\n\n    if dataset_version is not None:\n        metadata_url = f\"{metadata_url}/version/{dataset_version}\"\n\n    download_url = f\"{metadata_url}/download\"\n    upload_url = f\"{autodw_base}/ai-models/upload/single/{user_id}\"\n\n    try:\n        # --- 1. Fetch dataset metadata ---\n        logger.debug(f\"Fetching dataset metadata: {metadata_url}\")\n        metadata_response = requests.get(metadata_url, timeout=15)\n        metadata_response.raise_for_status()\n        metadata = metadata_response.json()\n\n        file_type = metadata.get(\"file_type\")\n        original_filename = metadata.get(\"original_filename\", \"train.csv\")\n\n        if file_type not in {\"csv\", \"tsv\", \"parquet\"}:\n            return JSONResponse(\n                status_code=400,\n                content={\"error\": f\"Unsupported file type '{file_type}'.\"},\n            )\n\n        # --- 2. Download dataset file ---\n        logger.debug(f\"Downloading dataset file: {download_url}\")\n        with requests.get(download_url, stream=True, timeout=30) as resp:\n            resp.raise_for_status()\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                tmp_path = Path(tmp_dir)\n                dataset_path = tmp_path / original_filename\n\n                with open(dataset_path, \"wb\") as f:\n                    for chunk in resp.iter_content(8192):\n                        f.write(chunk)\n\n                logger.info(f\"Dataset saved to {dataset_path}\")\n\n                # --- 3. Validate inputs ---\n                validation_error = validate_tabular_inputs(\n                    train_path=dataset_path,\n                    target_column_name=target_column_name,\n                    time_stamp_column_name=time_stamp_column_name,\n                    task_type=task_type,\n                )\n                if validation_error:\n                    return JSONResponse(\n                        status_code=400, content={\"error\": validation_error}\n                    )\n\n                # --- 4. Train AutoML model ---\n                save_model_path = tmp_path / \"automl_model\"\n                os.makedirs(save_model_path, exist_ok=True)\n\n                trainer = AutoMLTrainer(save_model_path=save_model_path)\n                train_df = load_table(dataset_path)\n\n                leaderboard, predictor = trainer.train(\n                    train_df=train_df,\n                    test_df=None,\n                    target_column=target_column_name,\n                    time_limit=int(time_budget),\n                )\n\n                # --- 5. Serialize predictor ---\n                predictor_path = save_model_path / \"predictor.pkl\"\n                with open(predictor_path, \"wb\") as f:\n                    pickle.dump(predictor, f)\n\n                zip_path = tmp_path / \"automl_predictor.zip\"\n                _ = shutil.make_archive(\n                    base_name=str(zip_path).replace(\".zip\", \"\"),\n                    format=\"zip\",\n                    root_dir=save_model_path,\n                )\n\n                leaderboard_json, leaderboard_str = convert_leaderboard_safely(\n                    leaderboard\n                )\n\n                # --- 6. Upload trained model to AutoDW ---\n                model_id = f\"automl_{dataset_id}_{int(datetime.utcnow().timestamp())}\"\n                # Get X-Task-ID from request headers if present\n                task_id = request.headers.get(\"X-Task-ID\")\n                headers = {}\n                if task_id:\n                    headers[\"X-Task-ID\"] = task_id\n                    logger.debug(f\"Including X-Task-ID header: {task_id}\")\n\n\n                with open(zip_path, \"rb\") as f:\n                    files = {\"file\": (zip_path.name, f, \"application/octet-stream\")}\n                    data = {\n                        \"model_id\": model_id,\n                        \"name\": f\"AutoML Model - {model_id}\",\n                        \"description\": \"AutoML trained model for tabular data\",\n                        \"framework\": \"sklearn\",\n                        \"model_type\": task_type,\n                        \"training_dataset\": str(dataset_id),\n                        \"leaderboard\": json.dumps(leaderboard_json),  # ensure JSON-safe\n                    }\n\n                    logger.debug(f\"Uploading model to {upload_url}\")\n                    upload_resp = requests.post(\n                        upload_url, headers=headers, files=files, data=data, timeout=120\n                    )\n\n                    if upload_resp.status_code &gt;= 400:\n                        logger.error(f\"Model upload failed: {upload_resp.text}\")\n                        return JSONResponse(\n                            status_code=upload_resp.status_code,\n                            content={\n                                \"error\": f\"Failed to upload model: {upload_resp.text}\"\n                            },\n                        )\n\n        logger.info(\"AutoML training completed and model uploaded successfully.\")\n        return JSONResponse(\n            status_code=200,\n            content={\n                \"message\": \"AutoML training completed successfully and model uploaded to AutoDW\",\n                \"leaderboard\": leaderboard_str,\n            },\n        )\n\n    except requests.RequestException as e:\n        logger.exception(\"Network or HTTP error during AutoDW communication\")\n        return JSONResponse(\n            status_code=502, content={\"error\": f\"AutoDW communication failed: {e}\"}\n        )\n    except Exception as e:\n        logger.exception(\"Unexpected error during AutoML training or upload\")\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.models.TabularSupervisedClassificationTask","title":"<code>TabularSupervisedClassificationTask</code>","text":"<p>               Bases: <code>TabularTask</code></p> <p>Tabular classification task configuration.</p> <p>Typical use-cases: churn prediction, loan approval, disease type, etc.</p> Source code in <code>app/tabular_automl/models.py</code> <pre><code>class TabularSupervisedClassificationTask(TabularTask):\n    \"\"\"Tabular classification task configuration.\n\n    Typical use-cases: churn prediction, loan approval, disease type, etc.\n    \"\"\"\n\n    task_type: str = \"classification\"\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.models.TabularSupervisedRegressionTask","title":"<code>TabularSupervisedRegressionTask</code>","text":"<p>               Bases: <code>TabularTask</code></p> <p>Tabular regression task configuration.</p> <p>Predicts continuous numeric values (e.g., price, salary, demand).</p> Source code in <code>app/tabular_automl/models.py</code> <pre><code>class TabularSupervisedRegressionTask(TabularTask):\n    \"\"\"Tabular regression task configuration.\n\n    Predicts continuous numeric values (e.g., price, salary, demand).\n    \"\"\"\n\n    task_type: str = \"regression\"\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.models.TabularSupervisedTimeSeriesTask","title":"<code>TabularSupervisedTimeSeriesTask</code>","text":"<p>               Bases: <code>TabularTask</code></p> <p>Time-series forecasting task configuration for tabular data.</p> Source code in <code>app/tabular_automl/models.py</code> <pre><code>class TabularSupervisedTimeSeriesTask(TabularTask):\n    \"\"\"Time-series forecasting task configuration for tabular data.\"\"\"\n\n    task_type: str = \"time_series\"\n    time_stamp_col: str = \"timestamp\"\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.models.TabularTask","title":"<code>TabularTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base Pydantic model describing common tabular task inputs.</p> Source code in <code>app/tabular_automl/models.py</code> <pre><code>class TabularTask(BaseModel):\n    \"\"\"Base Pydantic model describing common tabular task inputs.\"\"\"\n\n    target_feature: str\n    time_stamp_col: pd.DataFrame | None = None\n    train_file_path: Path\n    test_file_path: Path | None = None\n\n    class Config:\n        arbitrary_types_allowed: bool = True\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.modules.AutoMLTrainer","title":"<code>AutoMLTrainer</code>","text":"<p>Wrapper around AutoGluon Tabular training routines.</p> Source code in <code>app/tabular_automl/modules.py</code> <pre><code>class AutoMLTrainer:\n    \"\"\"Wrapper around AutoGluon Tabular training routines.\"\"\"\n\n    def __init__(\n        self,\n        save_model_path: Path,\n        DatasetClass=TabularDataset,\n        PredictorClass=TabularPredictor,\n    ):\n        self.save_model_path: Path = Path(save_model_path)\n        self.DatasetClass = DatasetClass\n        self.PredictorClass = PredictorClass\n        logger.debug(f\"Automl trainer, model path {self.save_model_path}\")\n\n    def train(\n        self,\n        train_df: pd.DataFrame,\n        test_df: pd.DataFrame | None,\n        target_column: str,\n        time_limit: int,\n    ) -&gt; tuple[pd.DataFrame | str, TabularPredictor] | str:\n        \"\"\"Train AutoGluon Tabular and return leaderboard or error.\"\"\"\n        final_train_df, final_test_df = self.train_test_split(\n            test_df=test_df, train_df=train_df\n        )\n\n        train_dataset = self.DatasetClass(final_train_df)\n        test_dataset = self.DatasetClass(final_test_df)\n\n        predictor = self.PredictorClass(\n            label=target_column, path=str(self.save_model_path)\n        ).fit(train_data=train_dataset, time_limit=time_limit)\n\n        save_path_clone_opt = self.save_model_path / \"-clone-opt\"\n        path_clone_opt = predictor.clone_for_deployment(path=str(save_path_clone_opt))\n        predictor_clone_opt = self.PredictorClass.load(path=str(path_clone_opt))\n\n        try:\n            return predictor.leaderboard(test_dataset), predictor_clone_opt\n        except Exception as e:\n            logger.error(f\"AutoML trainer failed {e}\")\n            return str(e)\n\n    def train_test_split(\n        self, test_df: pd.DataFrame | None, train_df: pd.DataFrame | None = None\n    ):\n        if test_df is None:\n            logger.debug(\"Test dataset not found, creating split\")\n            if train_df is not None:\n                final_train_df = train_df.sample(\n                    frac=DEFAULT_TABULAR_TRAIN_TEST_SPLIT_SIZE, random_state=42\n                )\n                final_test_df = train_df.drop(index=final_train_df.index.tolist())\n            else:\n                logger.error(\"Train df is empty\")\n                return str(\"Train df is empty\")\n        else:\n            logger.debug(\"Test dataset found\")\n            final_train_df = train_df\n            final_test_df = test_df\n        return final_train_df, final_test_df\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.modules.AutoMLTrainer.train","title":"<code>train(train_df, test_df, target_column, time_limit)</code>","text":"<p>Train AutoGluon Tabular and return leaderboard or error.</p> Source code in <code>app/tabular_automl/modules.py</code> <pre><code>def train(\n    self,\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame | None,\n    target_column: str,\n    time_limit: int,\n) -&gt; tuple[pd.DataFrame | str, TabularPredictor] | str:\n    \"\"\"Train AutoGluon Tabular and return leaderboard or error.\"\"\"\n    final_train_df, final_test_df = self.train_test_split(\n        test_df=test_df, train_df=train_df\n    )\n\n    train_dataset = self.DatasetClass(final_train_df)\n    test_dataset = self.DatasetClass(final_test_df)\n\n    predictor = self.PredictorClass(\n        label=target_column, path=str(self.save_model_path)\n    ).fit(train_data=train_dataset, time_limit=time_limit)\n\n    save_path_clone_opt = self.save_model_path / \"-clone-opt\"\n    path_clone_opt = predictor.clone_for_deployment(path=str(save_path_clone_opt))\n    predictor_clone_opt = self.PredictorClass.load(path=str(path_clone_opt))\n\n    try:\n        return predictor.leaderboard(test_dataset), predictor_clone_opt\n    except Exception as e:\n        logger.error(f\"AutoML trainer failed {e}\")\n        return str(e)\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.SessionData","title":"<code>SessionData</code>  <code>dataclass</code>","text":"<p>Lightweight container for session metadata retrieved from DB.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>@dataclass\nclass SessionData:\n    \"\"\"Lightweight container for session metadata retrieved from DB.\"\"\"\n\n    session_id: str\n    train_file_path: str\n    test_file_path: str | None\n    target_column: str\n    time_stamp_column_name: str | None\n    task_type: str\n    time_budget: int\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.create_session_directory","title":"<code>create_session_directory(upload_root=UPLOAD_ROOT)</code>","text":"<p>Create and return a new session id and directory path.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>def create_session_directory(upload_root: Path = UPLOAD_ROOT) -&gt; tuple[str, Path]:\n    \"\"\"Create and return a new session id and directory path.\"\"\"\n    session_id = str(uuid.uuid4())\n    session_dir = upload_root / session_id\n    session_dir.mkdir(parents=True, exist_ok=True)\n    logging.debug(f\"Session directory created at {session_dir}\")\n    return session_id, session_dir\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.get_session","title":"<code>get_session(session_id)</code>","text":"<p>Fetch a session by id, returning typed <code>SessionData</code> or None.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>def get_session(session_id: str) -&gt; SessionData | None:\n    \"\"\"Fetch a session by id, returning typed `SessionData` or None.\"\"\"\n    db = SessionLocal()\n    try:\n        rec = db.query(AutoMLSession).filter_by(session_id=session_id).first()\n        if rec is None:\n            return None\n        tb_raw = rec.__dict__.get(\"time_budget\")\n        return SessionData(\n            session_id=str(rec.session_id),\n            train_file_path=str(rec.train_file_path),\n            test_file_path=(\n                str(rec.test_file_path) if rec.test_file_path is not \"\" else None\n            ),\n            target_column=str(rec.target_column),\n            time_stamp_column_name=(\n                str(rec.time_stamp_column_name)\n                if rec.time_stamp_column_name is not \"\"\n                else None\n            ),\n            task_type=str(rec.task_type),\n            time_budget=int(tb_raw) if tb_raw is not None else 0,\n        )\n    finally:\n        db.close()\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.load_table","title":"<code>load_table(file_path)</code>","text":"<p>Load a table file into a DataFrame based on file extension.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>def load_table(file_path: Path) -&gt; pd.DataFrame:\n    \"\"\"Load a table file into a DataFrame based on file extension.\"\"\"\n    suffix = file_path.suffix.lower()\n    if suffix in [\".csv\"]:\n        logging.debug(f\"csv file loaded\")\n        return pd.read_csv(file_path)\n    if suffix in [\".xls\", \".xlsx\", \".xlsm\", \".xlsb\"]:\n        logging.debug(f\"excel file loaded\")\n        return pd.read_excel(file_path)\n    if suffix in [\".parquet\", \".pq\"]:\n        logging.debug(f\"Parquet file loaded\")\n        return pd.read_parquet(file_path)\n    if suffix in [\".json\"]:\n        logging.debug(f\"Json file loaded\")\n        return pd.read_json(file_path)\n    # Fallback: try csv to keep previous behavior\n    return pd.read_csv(file_path)\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.save_upload","title":"<code>save_upload(file, destination)</code>","text":"<p>Persist an uploaded file to the given destination path.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>def save_upload(file: UploadFile, destination: Path) -&gt; None:\n    \"\"\"Persist an uploaded file to the given destination path.\"\"\"\n    with open(destination, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n        logging.debug(f\"File saved to {destination}\")\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.store_session_in_db","title":"<code>store_session_in_db(session_id, train_path, test_path, target_column_name, time_stamp_column_name, task_type, time_budget)</code>","text":"<p>Persist a new AutoML session in the database.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>def store_session_in_db(\n    session_id: str,\n    train_path: Path,\n    test_path: Path | None,\n    target_column_name: str,\n    time_stamp_column_name: str | None,\n    task_type: str,\n    time_budget: int,\n) -&gt; None:\n    \"\"\"Persist a new AutoML session in the database.\"\"\"\n    db = SessionLocal()\n    try:\n        new_session = AutoMLSession(\n            session_id=session_id,\n            train_file_path=str(train_path),\n            test_file_path=str(test_path) if test_path else None,\n            target_column=target_column_name,\n            time_stamp_column_name=time_stamp_column_name,\n            task_type=task_type,\n            time_budget=time_budget,\n        )\n        db.add(new_session)\n        db.commit()\n    except Exception:\n        db.rollback()\n        raise\n    finally:\n        db.close()\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.services.validate_tabular_inputs","title":"<code>validate_tabular_inputs(train_path, target_column_name, time_stamp_column_name=None, task_type='classification')</code>","text":"<p>Validate required columns and task type for tabular training.</p> Source code in <code>app/tabular_automl/services.py</code> <pre><code>def validate_tabular_inputs(\n    train_path: Path,\n    target_column_name: str,\n    time_stamp_column_name: str | None = None,\n    task_type: str = \"classification\",\n) -&gt; str | None:\n    \"\"\"Validate required columns and task type for tabular training.\"\"\"\n    try:\n        train_df = load_table(train_path)\n    except Exception as e:\n        logging.error(f\"Could not read training data {e}\")\n        return f\"Could not read training data: {e}\"\n\n    if target_column_name not in train_df.columns:\n        logger.error(f\"Target column '{target_column_name}' not found.\")\n        return f\"Target column '{target_column_name}' not found.\"\n\n    if time_stamp_column_name and time_stamp_column_name not in train_df.columns:\n        logger.error(f\"Timestampl column '{time_stamp_column_name}' not found.\")\n        return f\"Timestamp column '{time_stamp_column_name}' not found.\"\n\n    if task_type not in [\"classification\", \"regression\", \"time series\"]:\n        logger.error(f\"Invalid task type {task_type}\")\n        return f\"Invalid task_type '{task_type}'\"\n\n    return None\n</code></pre>"},{"location":"api/tabular_automl/#app.tabular_automl.db.AutoMLSession","title":"<code>AutoMLSession</code>","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy model representing a tabular AutoML session.</p> <p>Stores file paths, task configuration, and creation time for a session.</p> Source code in <code>app/tabular_automl/db.py</code> <pre><code>class AutoMLSession(Base):\n    \"\"\"SQLAlchemy model representing a tabular AutoML session.\n\n    Stores file paths, task configuration, and creation time for a session.\n    \"\"\"\n\n    __tablename__ = \"automl_sessions\"\n\n    session_id = Column(String, primary_key=True, index=True)\n    train_file_path = Column(String, nullable=False)\n    test_file_path = Column(String, nullable=True)\n    target_column = Column(String, nullable=False)\n    time_stamp_column_name = Column(String, nullable=True)\n    task_type = Column(String, nullable=False)\n    time_budget = Column(Integer, nullable=False)\n    created_at = Column(DateTime, default=datetime.datetime.utcnow)\n</code></pre>"},{"location":"api/vision_automl/","title":"Vision AutoML","text":"<p>FastAPI endpoints for vision AutoML workflows.</p> <p>Handles session intake (CSV + images), validation, storage, model selection from Hugging Face Hub, and time-budgeted training.</p>"},{"location":"api/vision_automl/#app.vision_automl.models.ImageClassificationTask","title":"<code>ImageClassificationTask</code>","text":"<p>               Bases: <code>ImageTask</code></p> <p>Configuration for single-label image classification tasks.</p> Source code in <code>app/vision_automl/models.py</code> <pre><code>class ImageClassificationTask(ImageTask):\n    \"\"\"Configuration for single-label image classification tasks.\"\"\"\n\n    task_type: str = \"image_classification\"\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.models.ImageMultiLabelClassificationTask","title":"<code>ImageMultiLabelClassificationTask</code>","text":"<p>               Bases: <code>ImageTask</code></p> <p>Configuration for multi-label image classification tasks.</p> Source code in <code>app/vision_automl/models.py</code> <pre><code>class ImageMultiLabelClassificationTask(ImageTask):\n    \"\"\"Configuration for multi-label image classification tasks.\"\"\"\n\n    task_type: str = \"image_multilabel_classification\"\n    label_format: Literal[\"csv\", \"json\"] = \"csv\"  # required\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.models.ImageRegressionTask","title":"<code>ImageRegressionTask</code>","text":"<p>               Bases: <code>ImageTask</code></p> <p>Configuration for image regression tasks (predict numeric values).</p> Source code in <code>app/vision_automl/models.py</code> <pre><code>class ImageRegressionTask(ImageTask):\n    \"\"\"Configuration for image regression tasks (predict numeric values).\"\"\"\n\n    task_type: str = \"image_regression\"\n    label_format: Literal[\"csv\"] = \"csv\"  # regression needs exact values\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.models.ImageTask","title":"<code>ImageTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base Pydantic model describing common image task inputs.</p> Source code in <code>app/vision_automl/models.py</code> <pre><code>class ImageTask(BaseModel):\n    \"\"\"Base Pydantic model describing common image task inputs.\"\"\"\n\n    train_dir: Path\n    test_dir: Path | None = None\n    label_format: Literal[\"folder\", \"csv\"] = \"folder\"\n    labels_file: Path | None = None  # used if label_format != 'folder'\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.datamodule.ClassificationData","title":"<code>ClassificationData</code>","text":"<p>Handles dataset preparation and dataloaders for image classification tasks.</p> Source code in <code>app/vision_automl/ml_engine/datamodule.py</code> <pre><code>class ClassificationData:\n    \"\"\"Handles dataset preparation and dataloaders for image classification tasks.\"\"\"\n\n    def __init__(\n        self,\n        csv_file: str,\n        root_dir: str,\n        img_col: str = \"filename\",\n        label_col: str = \"label\",\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        num_workers: int = DEFAULT_NUM_WORKERS,\n        transform: Callable | None = None,\n        shuffle: bool = True,\n        val_split: float = DEFAULT_VAL_SPLIT,\n        test_split: float = DEFAULT_TEST_SPLIT,\n        seed: int = 42,\n        hf_model_id: str = DEFAULT_IMAGE_CLASSIFIER_HF_ID,\n    ) -&gt; None:\n        self.csv_file: str = csv_file\n        self.root_dir: str = root_dir\n        self.img_col: str = img_col\n        self.label_col: str = label_col\n        self.batch_size: int = batch_size\n        self.num_workers: int = num_workers\n        self.transform: Callable | None = transform\n        self.shuffle: bool = shuffle\n        self.val_split: float = val_split\n        self.test_split: float = test_split\n        self.seed: int = seed\n        self.hf_model_id: str = hf_model_id\n\n        self.num_classes: int = 0\n        self.train_dataset: ImageClassificationFromCSVDataset | None = None\n        self.val_dataset: ImageClassificationFromCSVDataset | None = None\n        self.test_dataset: ImageClassificationFromCSVDataset | None = None\n        self.processor: AutoImageProcessor | None = None\n        self.id2label: dict[int, str] = {}\n        self.label2id: dict[str, int] = {}\n\n        logger.info(\"Initializing ClassificationData with CSV: %s\", csv_file)\n        self.setup()\n\n    def setup(self) -&gt; None:\n        \"\"\"Create train/val/test splits, datasets, label maps, and processor.\"\"\"\n        logger.info(\"Reading dataset from %s\", self.csv_file)\n        df: pd.DataFrame = pd.read_csv(self.csv_file)\n\n        logger.debug(\"Initial dataset shape: %s\", df.shape)\n\n        # --- Split train, val, test ---\n        logger.info(\"Splitting dataset into train/val/test sets...\")\n        train_df, temp_df = train_test_split(\n            df,\n            test_size=self.val_split + self.test_split,\n            stratify=df[self.label_col],\n            random_state=self.seed,\n        )\n\n        relative_val = self.val_split / (self.val_split + self.test_split)\n        val_df, test_df = train_test_split(\n            temp_df,\n            test_size=1 - relative_val,\n            stratify=temp_df[self.label_col],\n            random_state=self.seed,\n        )\n\n        logger.info(\n            \"Split completed: train=%d, val=%d, test=%d\",\n            len(train_df),\n            len(val_df),\n            len(test_df),\n        )\n\n        # --- Build datasets ---\n        self.train_dataset = ImageClassificationFromCSVDataset(\n            csv_file=train_df,\n            root_dir=self.root_dir,\n            img_col=self.img_col,\n            label_col=self.label_col,\n            transform=self.transform,\n        )\n        self.val_dataset = ImageClassificationFromCSVDataset(\n            csv_file=val_df,\n            root_dir=self.root_dir,\n            img_col=self.img_col,\n            label_col=self.label_col,\n            transform=self.transform,\n        )\n        self.test_dataset = ImageClassificationFromCSVDataset(\n            csv_file=test_df,\n            root_dir=self.root_dir,\n            img_col=self.img_col,\n            label_col=self.label_col,\n            transform=self.transform,\n        )\n\n        # --- Label mapping ---\n        self.num_classes = len(self.train_dataset.classes)\n        self.id2label = {i: c for i, c in enumerate(self.train_dataset.classes)}\n        self.label2id = {c: i for i, c in enumerate(self.train_dataset.classes)}\n\n        logger.info(\"Number of classes detected: %d\", self.num_classes)\n        logger.debug(\"Label mappings: %s\", self.label2id)\n\n        # --- Processor ---\n        self.processor = AutoImageProcessor.from_pretrained(self.hf_model_id)\n        logger.info(\"Loaded processor from: %s\", self.hf_model_id)\n\n    def _collate_fn(self, batch: list[tuple[Any, Any]]) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Collate batch using HF processor to produce pixel values and labels.\"\"\"\n        images, labels = zip(*batch)\n        if self.processor is None:\n            logger.error(\"Processor not initialized. Call setup() first.\")\n            raise RuntimeError(\"Processor not initialized. Call setup() first.\")\n\n        pixel_values = self.processor(\n            images=list(images), return_tensors=\"pt\"\n        ).pixel_values\n        logger.debug(\"Collated batch with %d samples\", len(labels))\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": torch.tensor(labels, dtype=torch.long),\n        }\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Return training dataloader.\"\"\"\n        if self.train_dataset is None:\n            raise RuntimeError(\"Train dataset not initialized. Call setup() first.\")\n        logger.info(\"Creating training dataloader (batch_size=%d)\", self.batch_size)\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            num_workers=self.num_workers,\n            collate_fn=self._collate_fn,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Return validation dataloader.\"\"\"\n        if self.val_dataset is None:\n            raise RuntimeError(\n                \"Validation dataset not initialized. Call setup() first.\"\n            )\n        logger.info(\"Creating validation dataloader (batch_size=%d)\", self.batch_size)\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            collate_fn=self._collate_fn,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Return test dataloader.\"\"\"\n        if self.test_dataset is None:\n            raise RuntimeError(\"Test dataset not initialized. Call setup() first.\")\n        logger.info(\"Creating test dataloader (batch_size=%d)\", self.batch_size)\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            collate_fn=self._collate_fn,\n        )\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.datamodule.ClassificationData.setup","title":"<code>setup()</code>","text":"<p>Create train/val/test splits, datasets, label maps, and processor.</p> Source code in <code>app/vision_automl/ml_engine/datamodule.py</code> <pre><code>def setup(self) -&gt; None:\n    \"\"\"Create train/val/test splits, datasets, label maps, and processor.\"\"\"\n    logger.info(\"Reading dataset from %s\", self.csv_file)\n    df: pd.DataFrame = pd.read_csv(self.csv_file)\n\n    logger.debug(\"Initial dataset shape: %s\", df.shape)\n\n    # --- Split train, val, test ---\n    logger.info(\"Splitting dataset into train/val/test sets...\")\n    train_df, temp_df = train_test_split(\n        df,\n        test_size=self.val_split + self.test_split,\n        stratify=df[self.label_col],\n        random_state=self.seed,\n    )\n\n    relative_val = self.val_split / (self.val_split + self.test_split)\n    val_df, test_df = train_test_split(\n        temp_df,\n        test_size=1 - relative_val,\n        stratify=temp_df[self.label_col],\n        random_state=self.seed,\n    )\n\n    logger.info(\n        \"Split completed: train=%d, val=%d, test=%d\",\n        len(train_df),\n        len(val_df),\n        len(test_df),\n    )\n\n    # --- Build datasets ---\n    self.train_dataset = ImageClassificationFromCSVDataset(\n        csv_file=train_df,\n        root_dir=self.root_dir,\n        img_col=self.img_col,\n        label_col=self.label_col,\n        transform=self.transform,\n    )\n    self.val_dataset = ImageClassificationFromCSVDataset(\n        csv_file=val_df,\n        root_dir=self.root_dir,\n        img_col=self.img_col,\n        label_col=self.label_col,\n        transform=self.transform,\n    )\n    self.test_dataset = ImageClassificationFromCSVDataset(\n        csv_file=test_df,\n        root_dir=self.root_dir,\n        img_col=self.img_col,\n        label_col=self.label_col,\n        transform=self.transform,\n    )\n\n    # --- Label mapping ---\n    self.num_classes = len(self.train_dataset.classes)\n    self.id2label = {i: c for i, c in enumerate(self.train_dataset.classes)}\n    self.label2id = {c: i for i, c in enumerate(self.train_dataset.classes)}\n\n    logger.info(\"Number of classes detected: %d\", self.num_classes)\n    logger.debug(\"Label mappings: %s\", self.label2id)\n\n    # --- Processor ---\n    self.processor = AutoImageProcessor.from_pretrained(self.hf_model_id)\n    logger.info(\"Loaded processor from: %s\", self.hf_model_id)\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.datamodule.ClassificationData.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Return test dataloader.</p> Source code in <code>app/vision_automl/ml_engine/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Return test dataloader.\"\"\"\n    if self.test_dataset is None:\n        raise RuntimeError(\"Test dataset not initialized. Call setup() first.\")\n    logger.info(\"Creating test dataloader (batch_size=%d)\", self.batch_size)\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        collate_fn=self._collate_fn,\n    )\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.datamodule.ClassificationData.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Return training dataloader.</p> Source code in <code>app/vision_automl/ml_engine/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Return training dataloader.\"\"\"\n    if self.train_dataset is None:\n        raise RuntimeError(\"Train dataset not initialized. Call setup() first.\")\n    logger.info(\"Creating training dataloader (batch_size=%d)\", self.batch_size)\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=self.shuffle,\n        num_workers=self.num_workers,\n        collate_fn=self._collate_fn,\n    )\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.datamodule.ClassificationData.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Return validation dataloader.</p> Source code in <code>app/vision_automl/ml_engine/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Return validation dataloader.\"\"\"\n    if self.val_dataset is None:\n        raise RuntimeError(\n            \"Validation dataset not initialized. Call setup() first.\"\n        )\n    logger.info(\"Creating validation dataloader (batch_size=%d)\", self.batch_size)\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        collate_fn=self._collate_fn,\n    )\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.dataset.ImageClassificationFromCSVDataset","title":"<code>ImageClassificationFromCSVDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Torch dataset that reads image paths and labels from a CSV/DataFrame.</p> Source code in <code>app/vision_automl/ml_engine/dataset.py</code> <pre><code>class ImageClassificationFromCSVDataset(Dataset):\n    \"\"\"Torch dataset that reads image paths and labels from a CSV/DataFrame.\"\"\"\n\n    def __init__(\n        self,\n        csv_file: Union[str, pd.DataFrame],\n        root_dir: str,\n        img_col: str = \"image\",\n        label_col: str = \"label\",\n        transform: Optional[T.Compose] = None,\n    ):\n        if isinstance(csv_file, str):\n            self.label_csv = pd.read_csv(csv_file)\n        elif isinstance(csv_file, pd.DataFrame):\n            self.label_csv = csv_file.reset_index(drop=True)\n        else:\n            raise ValueError(\"csv_file must be a path or DataFrame\")\n\n        self.root_dir = root_dir\n        self.img_col = img_col\n        self.label_col = label_col\n        # By default, do not apply torchvision transforms so that a Hugging Face\n        # AutoImageProcessor can handle preprocessing in a DataLoader collate_fn.\n        self.transform = transform\n\n        if self.label_csv[self.label_col].dtype not in [int, float]:\n            self.classes = sorted(self.label_csv[self.label_col].unique().tolist())\n            self.class_to_idx = {\n                cls_name: idx for idx, cls_name in enumerate(self.classes)\n            }\n            self.idx_to_class = {\n                idx: cls_name for cls_name, idx in self.class_to_idx.items()\n            }\n            self.label_csv[self.label_col] = self.label_csv[self.label_col].map(\n                self.class_to_idx\n            )\n        else:\n            self.classes = sorted(self.label_csv[self.label_col].unique().tolist())\n            self.class_to_idx = {cls: cls for cls in self.classes}\n            self.idx_to_class = {idx: cls for cls in self.classes}\n\n    def __len__(self):\n        \"\"\"Return number of samples.\"\"\"\n        return len(self.label_csv)\n\n    def __getitem__(self, idx):\n        \"\"\"Return a single sample as (image, label).\"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.item()\n        label = int(self.label_csv.iloc[idx][self.label_col])\n\n        # Handle both flat and hierarchical directory structures\n        filename = self.label_csv.iloc[idx][self.img_col]\n        label_name = self.idx_to_class[label]\n\n        # First try: images are in subdirectories by label (e.g., root_dir/label/filename)\n        img_path = os.path.join(self.root_dir, label_name, filename)\n\n        # If that doesn't exist, try flat structure (e.g., root_dir/filename)\n        if not os.path.exists(img_path):\n            img_path = os.path.join(self.root_dir, filename)\n\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, torch.tensor(label, dtype=torch.long)\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.dataset.ImageClassificationFromCSVDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return a single sample as (image, label).</p> Source code in <code>app/vision_automl/ml_engine/dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"Return a single sample as (image, label).\"\"\"\n    if torch.is_tensor(idx):\n        idx = idx.item()\n    label = int(self.label_csv.iloc[idx][self.label_col])\n\n    # Handle both flat and hierarchical directory structures\n    filename = self.label_csv.iloc[idx][self.img_col]\n    label_name = self.idx_to_class[label]\n\n    # First try: images are in subdirectories by label (e.g., root_dir/label/filename)\n    img_path = os.path.join(self.root_dir, label_name, filename)\n\n    # If that doesn't exist, try flat structure (e.g., root_dir/filename)\n    if not os.path.exists(img_path):\n        img_path = os.path.join(self.root_dir, filename)\n\n    img = Image.open(img_path).convert(\"RGB\")\n    if self.transform:\n        img = self.transform(img)\n    return img, torch.tensor(label, dtype=torch.long)\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.dataset.ImageClassificationFromCSVDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of samples.</p> Source code in <code>app/vision_automl/ml_engine/dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return number of samples.\"\"\"\n    return len(self.label_csv)\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.model.ClassificationModel","title":"<code>ClassificationModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Thin wrapper over HF image classification model for logits output.</p> Source code in <code>app/vision_automl/ml_engine/model.py</code> <pre><code>class ClassificationModel(nn.Module):\n    \"\"\"Thin wrapper over HF image classification model for logits output.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str = \"google/vit-base-patch16-224\",\n        num_classes: int = 2,\n        id2label: dict | None = None,\n        label2id: dict | None = None,\n    ):\n        super().__init__()\n        config_kwargs = {\n            \"num_labels\": num_classes,\n            \"id2label\": id2label or {i: str(i) for i in range(num_classes)},\n            \"label2id\": label2id or {str(i): i for i in range(num_classes)},\n        }\n        self.model = AutoModelForImageClassification.from_pretrained(\n            model_id,\n            ignore_mismatched_sizes=True,\n            **config_kwargs,\n        )\n\n    def forward(self, pixel_values):\n        \"\"\"Forward pass returning raw classification logits.\"\"\"\n        outputs = self.model(pixel_values=pixel_values)\n        return outputs.logits\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.model.ClassificationModel.forward","title":"<code>forward(pixel_values)</code>","text":"<p>Forward pass returning raw classification logits.</p> Source code in <code>app/vision_automl/ml_engine/model.py</code> <pre><code>def forward(self, pixel_values):\n    \"\"\"Forward pass returning raw classification logits.\"\"\"\n    outputs = self.model(pixel_values=pixel_values)\n    return outputs.logits\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>Simple early stopping callback based on monitored metric.</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>class EarlyStopping:\n    \"\"\"Simple early stopping callback based on monitored metric.\"\"\"\n\n    def __init__(\n        self, monitor: str = \"val_loss\", patience: int = 3, min_delta: float = 0.0\n    ) -&gt; None:\n        self.monitor: str = monitor\n        self.patience: int = patience\n        self.min_delta: float = min_delta\n        self.best: float = float(\"inf\")\n        self.counter: int = 0\n\n    def on_epoch_end(\n        self, trainer: \"FabricTrainer\", epoch: int, logs: dict[str, float]\n    ) -&gt; None:\n        \"\"\"Update state after epoch; may signal stopping on trainer.\"\"\"\n        current: float | None = logs.get(self.monitor)\n        if current is None:\n            logger.warning(\n                f\"Metric '{self.monitor}' not found in logs. Skipping early stopping check.\"\n            )\n            return\n\n        if current &lt; self.best - self.min_delta:\n            self.best = current\n            self.counter = 0\n            logger.info(f\"New best {self.monitor}: {self.best:.4f}\")\n        else:\n            self.counter += 1\n            logger.info(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n            if self.counter &gt;= self.patience:\n                logger.info(\"Early stopping triggered!\")\n                trainer.epochs = epoch + 1\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.EarlyStopping.on_epoch_end","title":"<code>on_epoch_end(trainer, epoch, logs)</code>","text":"<p>Update state after epoch; may signal stopping on trainer.</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>def on_epoch_end(\n    self, trainer: \"FabricTrainer\", epoch: int, logs: dict[str, float]\n) -&gt; None:\n    \"\"\"Update state after epoch; may signal stopping on trainer.\"\"\"\n    current: float | None = logs.get(self.monitor)\n    if current is None:\n        logger.warning(\n            f\"Metric '{self.monitor}' not found in logs. Skipping early stopping check.\"\n        )\n        return\n\n    if current &lt; self.best - self.min_delta:\n        self.best = current\n        self.counter = 0\n        logger.info(f\"New best {self.monitor}: {self.best:.4f}\")\n    else:\n        self.counter += 1\n        logger.info(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n        if self.counter &gt;= self.patience:\n            logger.info(\"Early stopping triggered!\")\n            trainer.epochs = epoch + 1\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.FabricTrainer","title":"<code>FabricTrainer</code>","text":"<p>Minimal trainer using Lightning Fabric for classification tasks.</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>class FabricTrainer:\n    \"\"\"Minimal trainer using Lightning Fabric for classification tasks.\"\"\"\n\n    def __init__(\n        self,\n        datamodule: Any,\n        model_class: type[nn.Module],\n        model_kwargs: dict[str, Any] = {},\n        optimizer_class: type[optim.Optimizer] = optim.AdamW,\n        optimizer_kwargs: dict[str, Any] = {},\n        loss_fn: nn.Module = nn.CrossEntropyLoss(),\n        lr: float = 0.001,\n        epochs: int = 1,\n        time_limit: float | None = None,\n        device: str = \"auto\",\n        callbacks: list[Any] = [],\n        input_dtype: torch.dtype = torch.float32,\n        target_dtype: torch.dtype = torch.long,\n    ) -&gt; None:\n        self.datamodule: Any = datamodule\n        self.model_class: type[nn.Module] = model_class\n        self.model_kwargs: dict[str, Any] = model_kwargs\n        self.optimizer_class: type[optim.Optimizer] = optimizer_class\n        self.optimizer_kwargs: dict[str, Any] = optimizer_kwargs or {\"lr\": lr}\n        self.loss_fn: nn.Module = loss_fn\n        self.epochs: int = epochs\n        self.time_limit: float | None = time_limit\n        self.device: str = device\n        self.callbacks: list[Any] = callbacks\n        self.input_dtype: torch.dtype = input_dtype\n        self.target_dtype: torch.dtype = target_dtype\n\n        self.fabric: L.Fabric = L.Fabric(devices=self.device)\n        self._setup_model_optimizer()\n\n    def _setup_model_optimizer(self) -&gt; None:\n        \"\"\"Instantiate model and optimizer and prepare loaders with Fabric.\"\"\"\n        logger.info(\"Setting up model and optimizer.\")\n        self.model: nn.Module = self.model_class(**self.model_kwargs)\n        self.optimizer: optim.Optimizer = self.optimizer_class(\n            self.model.parameters(), **self.optimizer_kwargs\n        )\n\n        train_loader: Any = self.datamodule.train_dataloader()\n        val_loader: Any = self.datamodule.val_dataloader()\n        self.model, self.optimizer, self.train_loader, self.val_loader = (\n            self.fabric.setup(self.model, self.optimizer, train_loader, val_loader)\n        )\n        self.test_loader: Any = self.datamodule.test_dataloader()\n        logger.info(\"Model and optimizer setup complete.\")\n\n    def _move_batch(self, batch: Any) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Move batch tensors to device and standardize batch dict.\"\"\"\n        pixel_values: torch.Tensor\n        labels: torch.Tensor\n\n        if isinstance(batch, dict):\n            pixel_values = batch[\"pixel_values\"].to(\n                self.fabric.device, dtype=self.input_dtype\n            )\n            labels = batch[\"labels\"].to(self.fabric.device, dtype=self.target_dtype)\n        else:\n            imgs, batch_labels = batch\n            pixel_values = imgs.to(self.fabric.device, dtype=self.input_dtype)\n            labels = batch_labels.to(self.fabric.device, dtype=self.target_dtype)\n\n        moved_batch: dict[str, torch.Tensor] = {\n            \"pixel_values\": pixel_values,\n            \"labels\": labels,\n        }\n        return moved_batch\n\n    def _check_time_limit(self, start_time: float) -&gt; bool:\n        \"\"\"Return True if configured time limit has been exceeded.\"\"\"\n        elapsed: float = time.time() - start_time\n        if self.time_limit and elapsed &gt; self.time_limit:\n            logger.warning(f\"Time limit reached ({elapsed:.2f}s). Stopping training.\")\n            return True\n        return False\n\n    def train_epoch(self, epoch: int, start_time: float) -&gt; float:\n        \"\"\"Train for a single epoch and return average training loss.\"\"\"\n        self.model.train()\n        running_loss: float = 0.0\n        batch_count: int = len(self.train_loader)\n\n        for batch in tqdm(\n            self.train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False\n        ):\n            if self._check_time_limit(start_time):\n                return running_loss / max(1, batch_count)\n\n            moved: dict[str, torch.Tensor] = self._move_batch(batch)\n            self.optimizer.zero_grad()\n            outputs: torch.Tensor = self.model(moved[\"pixel_values\"])\n            loss: torch.Tensor = self.loss_fn(outputs, moved[\"labels\"])\n            self.fabric.backward(loss)\n            self.optimizer.step()\n            running_loss += loss.item()\n\n        avg_loss: float = running_loss / batch_count\n        logger.info(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n        return avg_loss\n\n    def validate(self, start_time: float) -&gt; tuple[float, float]:\n        \"\"\"Evaluate on validation set; return (avg_loss, accuracy).\"\"\"\n        self.model.eval()\n        val_loss: float = 0.0\n        correct: int = 0\n        total: int = 0\n\n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc=\"Validation\", leave=False):\n                if self._check_time_limit(start_time):\n                    break\n\n                moved: dict[str, torch.Tensor] = self._move_batch(batch)\n                outputs: torch.Tensor = self.model(moved[\"pixel_values\"])\n                loss: torch.Tensor = self.loss_fn(outputs, moved[\"labels\"])\n                val_loss += loss.item()\n\n                preds: torch.Tensor = outputs.argmax(dim=1)\n                correct += (preds == moved[\"labels\"]).sum().item()\n                total += moved[\"labels\"].size(0)\n\n        avg_loss: float = val_loss / max(1, len(self.val_loader))\n        accuracy: float = correct / max(1, total)\n        logger.info(f\"Validation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n        return avg_loss, accuracy\n\n    def test(self) -&gt; tuple[float, float]:\n        \"\"\"Evaluate on test set; return (avg_loss, accuracy).\"\"\"\n        self.model.eval()\n        test_loss: float = 0.0\n        correct: int = 0\n        total: int = 0\n\n        with torch.no_grad():\n            for batch in tqdm(self.test_loader, desc=\"Testing\"):\n                moved: dict[str, torch.Tensor] = self._move_batch(batch)\n                outputs: torch.Tensor = self.model(moved[\"pixel_values\"])\n                loss: torch.Tensor = self.loss_fn(outputs, moved[\"labels\"])\n                test_loss += loss.item()\n\n                preds: torch.Tensor = outputs.argmax(dim=1)\n                correct += (preds == moved[\"labels\"]).sum().item()\n                total += moved[\"labels\"].size(0)\n\n        avg_loss: float = test_loss / len(self.test_loader)\n        accuracy: float = correct / total\n        logger.info(f\"Test Results - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n        return avg_loss, accuracy\n\n    def fit(self) -&gt; tuple[float, float]:\n        \"\"\"Run the full training loop and return test metrics.\"\"\"\n        logger.info(\"Starting training loop.\")\n        start_time: float = time.time()\n\n        for epoch in range(self.epochs):\n            train_loss: float = self.train_epoch(epoch, start_time)\n            val_loss, val_acc = self.validate(start_time)\n            logs: dict[str, float] = {\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            }\n\n            for cb in self.callbacks:\n                cb.on_epoch_end(self, epoch, logs)\n\n            if self._check_time_limit(start_time):\n                break\n\n        logger.info(\"Training complete. Running test evaluation.\")\n        return self.test()\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.FabricTrainer.fit","title":"<code>fit()</code>","text":"<p>Run the full training loop and return test metrics.</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>def fit(self) -&gt; tuple[float, float]:\n    \"\"\"Run the full training loop and return test metrics.\"\"\"\n    logger.info(\"Starting training loop.\")\n    start_time: float = time.time()\n\n    for epoch in range(self.epochs):\n        train_loss: float = self.train_epoch(epoch, start_time)\n        val_loss, val_acc = self.validate(start_time)\n        logs: dict[str, float] = {\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n\n        for cb in self.callbacks:\n            cb.on_epoch_end(self, epoch, logs)\n\n        if self._check_time_limit(start_time):\n            break\n\n    logger.info(\"Training complete. Running test evaluation.\")\n    return self.test()\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.FabricTrainer.test","title":"<code>test()</code>","text":"<p>Evaluate on test set; return (avg_loss, accuracy).</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>def test(self) -&gt; tuple[float, float]:\n    \"\"\"Evaluate on test set; return (avg_loss, accuracy).\"\"\"\n    self.model.eval()\n    test_loss: float = 0.0\n    correct: int = 0\n    total: int = 0\n\n    with torch.no_grad():\n        for batch in tqdm(self.test_loader, desc=\"Testing\"):\n            moved: dict[str, torch.Tensor] = self._move_batch(batch)\n            outputs: torch.Tensor = self.model(moved[\"pixel_values\"])\n            loss: torch.Tensor = self.loss_fn(outputs, moved[\"labels\"])\n            test_loss += loss.item()\n\n            preds: torch.Tensor = outputs.argmax(dim=1)\n            correct += (preds == moved[\"labels\"]).sum().item()\n            total += moved[\"labels\"].size(0)\n\n    avg_loss: float = test_loss / len(self.test_loader)\n    accuracy: float = correct / total\n    logger.info(f\"Test Results - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n    return avg_loss, accuracy\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.FabricTrainer.train_epoch","title":"<code>train_epoch(epoch, start_time)</code>","text":"<p>Train for a single epoch and return average training loss.</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>def train_epoch(self, epoch: int, start_time: float) -&gt; float:\n    \"\"\"Train for a single epoch and return average training loss.\"\"\"\n    self.model.train()\n    running_loss: float = 0.0\n    batch_count: int = len(self.train_loader)\n\n    for batch in tqdm(\n        self.train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False\n    ):\n        if self._check_time_limit(start_time):\n            return running_loss / max(1, batch_count)\n\n        moved: dict[str, torch.Tensor] = self._move_batch(batch)\n        self.optimizer.zero_grad()\n        outputs: torch.Tensor = self.model(moved[\"pixel_values\"])\n        loss: torch.Tensor = self.loss_fn(outputs, moved[\"labels\"])\n        self.fabric.backward(loss)\n        self.optimizer.step()\n        running_loss += loss.item()\n\n    avg_loss: float = running_loss / batch_count\n    logger.info(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n    return avg_loss\n</code></pre>"},{"location":"api/vision_automl/#app.vision_automl.ml_engine.trainer.FabricTrainer.validate","title":"<code>validate(start_time)</code>","text":"<p>Evaluate on validation set; return (avg_loss, accuracy).</p> Source code in <code>app/vision_automl/ml_engine/trainer.py</code> <pre><code>def validate(self, start_time: float) -&gt; tuple[float, float]:\n    \"\"\"Evaluate on validation set; return (avg_loss, accuracy).\"\"\"\n    self.model.eval()\n    val_loss: float = 0.0\n    correct: int = 0\n    total: int = 0\n\n    with torch.no_grad():\n        for batch in tqdm(self.val_loader, desc=\"Validation\", leave=False):\n            if self._check_time_limit(start_time):\n                break\n\n            moved: dict[str, torch.Tensor] = self._move_batch(batch)\n            outputs: torch.Tensor = self.model(moved[\"pixel_values\"])\n            loss: torch.Tensor = self.loss_fn(outputs, moved[\"labels\"])\n            val_loss += loss.item()\n\n            preds: torch.Tensor = outputs.argmax(dim=1)\n            correct += (preds == moved[\"labels\"]).sum().item()\n            total += moved[\"labels\"].size(0)\n\n    avg_loss: float = val_loss / max(1, len(self.val_loader))\n    accuracy: float = correct / max(1, total)\n    logger.info(f\"Validation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n    return avg_loss, accuracy\n</code></pre>"}]}